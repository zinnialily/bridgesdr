{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a9248e-0d3b-4878-94e5-209b19a9dced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all\n",
      "model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_40988\\145022962.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================\n",
    "# Imports\n",
    "# ============================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Dataset Classes\n",
    "# ============================\n",
    "class XBDMulticlassDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_pre=None, transform_post=None,\n",
    "                 image_size=(1024, 1024), tile_size=(256, 256), max_images=None):\n",
    "        self.image_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.files = sorted([f for f in os.listdir(self.image_dir) if '_pre_disaster.png' in f])\n",
    "        if max_images:\n",
    "            self.files = self.files[:max_images]\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "        self.image_size = image_size\n",
    "        self.tile_size = tile_size\n",
    "        self.tiles_per_image = (image_size[0] // tile_size[0]) * (image_size[1] // tile_size[1])\n",
    "\n",
    "    def __getitem__(self, idx, return_raw=False):\n",
    "        pre_file = self.files[idx]\n",
    "        post_file = pre_file.replace('pre', 'post')\n",
    "        mask_file = post_file.replace('.png', '_rgb.png')\n",
    "        pre_path = os.path.join(self.image_dir, pre_file)\n",
    "        post_path = os.path.join(self.image_dir, post_file)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "\n",
    "        pre_img_raw = Image.open(pre_path).convert(\"RGB\").resize(self.image_size)\n",
    "        post_img_raw = Image.open(post_path).convert(\"RGB\").resize(self.image_size)\n",
    "        mask_img_raw = Image.open(mask_path).convert(\"RGB\").resize(self.image_size, Image.NEAREST)\n",
    "\n",
    "        def optical_to_sar_like(img):\n",
    "            img = img.convert('L')\n",
    "            img = ImageOps.autocontrast(img, cutoff=2)\n",
    "            return img\n",
    "\n",
    "        def update_mask_multiclass(mask_rgb_img):\n",
    "            mask_np = np.array(mask_rgb_img)\n",
    "            label_mask = np.zeros(mask_np.shape[:2], dtype=np.uint8)\n",
    "            color_to_label = {\n",
    "                (0, 0, 0): 0, (0, 255, 255): 0, (0, 0, 255): 1,\n",
    "                (255, 255, 0): 2, (255, 0, 0): 3, (211, 211, 211): 0\n",
    "            }\n",
    "            for rgb, label in color_to_label.items():\n",
    "                mask = np.all(mask_np == rgb, axis=-1)\n",
    "                label_mask[mask] = label\n",
    "            return label_mask\n",
    "\n",
    "        post_img_sar_raw = optical_to_sar_like(post_img_raw)\n",
    "        if return_raw:\n",
    "            mask_np = update_mask_multiclass(mask_img_raw)\n",
    "            return pre_img_raw, post_img_sar_raw, mask_np\n",
    "\n",
    "        pre_img = self.transform_pre(pre_img_raw) if self.transform_pre else pre_img_raw\n",
    "        post_img_sar = self.transform_post(post_img_sar_raw) if self.transform_post else post_img_sar_raw\n",
    "\n",
    "        input_tensor = torch.cat([pre_img, post_img_sar], dim=0)\n",
    "        mask_tensor = torch.tensor(update_mask_multiclass(mask_img_raw), dtype=torch.long)\n",
    "\n",
    "        return tile_tensor_and_mask(input_tensor, mask_tensor, self.tile_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def get_tile_dataset(self):\n",
    "        return TiledXBDDataset(self)\n",
    "\n",
    "class TiledXBDDataset(Dataset):\n",
    "    def __init__(self, parent_dataset):\n",
    "        self.parent_dataset = parent_dataset\n",
    "        self.tiles_per_image = parent_dataset.tiles_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.tiles_per_image\n",
    "        tile_idx = idx % self.tiles_per_image\n",
    "        tiles_input, tiles_mask = self.parent_dataset[image_idx]\n",
    "        return tiles_input[tile_idx], tiles_mask[tile_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent_dataset) * self.tiles_per_image\n",
    "\n",
    "class FilteredTileDataset(Dataset):\n",
    "    def __init__(self, tiled_dataset, keep_zero_damage_prob=0.1):\n",
    "        self.tiled_dataset = tiled_dataset\n",
    "        self.filtered_indices = []\n",
    "        for idx in range(len(tiled_dataset)):\n",
    "            _, mask = tiled_dataset[idx]\n",
    "            if mask.max() == 0:\n",
    "                if random.random() < keep_zero_damage_prob:\n",
    "                    self.filtered_indices.append(idx)\n",
    "            else:\n",
    "                self.filtered_indices.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tiled_dataset[self.filtered_indices[idx]]\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_majority_class_per_tile(tile_mask):\n",
    "    # Ignore padding or empty masks\n",
    "    return torch.bincount(tile_mask.flatten()).argmax().item()\n",
    "\n",
    "def stratified_tile_sample(dataset, total_samples=500):\n",
    "    class_indices = {0: [], 1: [], 2: [], 3: []}\n",
    "\n",
    "    print(\"Scanning tile classes for stratified sampling...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, mask = dataset[idx]\n",
    "        majority_class = get_majority_class_per_tile(mask)\n",
    "        if majority_class in class_indices:\n",
    "            class_indices[majority_class].append(idx)\n",
    "\n",
    "    # Count all\n",
    "    total_tiles = sum(len(v) for v in class_indices.values())\n",
    "    proportions = {cls: len(v)/total_tiles for cls, v in class_indices.items()}\n",
    "    print(\"Class proportions:\", proportions)\n",
    "\n",
    "    # Sample proportional counts\n",
    "    sampled_indices = []\n",
    "    for cls in range(4):\n",
    "        n_samples = int(proportions[cls] * total_samples)\n",
    "        n_available = len(class_indices[cls])\n",
    "        chosen = random.sample(class_indices[cls], min(n_samples, n_available))\n",
    "        sampled_indices.extend(chosen)\n",
    "\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# ============================\n",
    "# Helper Functions\n",
    "# ============================\n",
    "def tile_tensor_and_mask(input_tensor, mask_tensor, tile_size=(256, 256)):\n",
    "    C, H, W = input_tensor.shape\n",
    "    th, tw = tile_size\n",
    "    tiles_input, tiles_mask = [], []\n",
    "    for i in range(0, H, th):\n",
    "        for j in range(0, W, tw):\n",
    "            tiles_input.append(input_tensor[:, i:i+th, j:j+tw])\n",
    "            tiles_mask.append(mask_tensor[i:i+th, j:j+tw])\n",
    "    return tiles_input, tiles_mask\n",
    "\n",
    "# ============================\n",
    "# Visualization\n",
    "# ============================\n",
    "def unnormalize(img_tensor, mean, std):\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor\n",
    "\n",
    "def visualize_predictions(inputs, masks, outputs, n=4):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    cmap = mcolors.ListedColormap(['black', 'blue', 'yellow', 'red'])\n",
    "    bounds = [0, 1, 2, 3, 4]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    for i in range(min(n, inputs.shape[0])):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        pre_rgb = unnormalize(inputs[i][:3].clone(), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        axs[0].imshow(np.transpose(torch.clamp(pre_rgb, 0, 1).cpu().numpy(), (1, 2, 0)))\n",
    "        axs[0].set_title(\"Pre-disaster RGB\")\n",
    "        axs[0].axis('off')\n",
    "        axs[1].imshow(inputs[i][3].cpu().numpy(), cmap='gray')\n",
    "        axs[1].set_title(\"Post-disaster SAR\")\n",
    "        axs[1].axis('off')\n",
    "        axs[2].imshow(preds[i], cmap=cmap, norm=norm)\n",
    "        axs[2].set_title(\"Predicted\")\n",
    "        axs[2].axis('off')\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        plt.imshow(masks[i], cmap=cmap, norm=norm)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Saved all\")\n",
    "\n",
    "class UNetOriginal(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.final = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "model_path = 'C:/Users/sweta/anaconda_projects/non-trivial/performance_bias/iteration_models/model_epoch7.pt'\n",
    "model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "print(\"model loaded successfully\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transforms (same as training)\n",
    "transform_pre = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_post = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load full xBD dataset\n",
    "xbd_root = r\"C:\\Users\\sweta\\.cache\\kagglehub\\datasets\\qianlanzz\\xbd-dataset\\versions\\1\\xbd\\tier1\"\n",
    "xbd_dataset = XBDMulticlassDataset(\n",
    "    xbd_root, \n",
    "    transform_pre=transform_pre, \n",
    "    transform_post=transform_post,\n",
    "    image_size=(1024, 1024),\n",
    "    tile_size=(256, 256)\n",
    ")\n",
    "\n",
    "# Convert to per-tile dataset for evaluation\n",
    "xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "xbd_loader = DataLoader(xbd_tile_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def evaluate_model_on_xbd(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    results = []\n",
    "    save_dir = os.path.join(save_root, f\"xbd_{label}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(dataloader, desc=f\"Evaluating on xBD: {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for j in range(x.size(0)):\n",
    "                acc = compute_accuracy(y_pred[j], y[j])\n",
    "                iou = compute_iou(y_pred[j], y[j])\n",
    "                results.append({\n",
    "                    \"tile_index\": i * x.size(0) + j,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"iou\": iou\n",
    "                })\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_dir, f\"xbd_{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n[SUMMARY: xBD {label}]\")\n",
    "    print(df.describe())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d5a60d-9fee-489f-b510-654be4e0ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning tile classes for stratified sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44784/44784 [3:57:20<00:00,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class proportions: {0: 0.9993971061093248, 1: 0.0, 2: 0.00040192926045016077, 3: 0.00020096463022508038}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "stratified_subset = stratified_tile_sample(xbd_tile_dataset, total_samples=500)\n",
    "xbd_loader = DataLoader(stratified_subset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f87a69-979f-4299-aec1-839bda045628",
   "metadata": {},
   "source": [
    "LIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30dd9c2-e54e-481b-ad07-8dcab73df2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_26712\\3367217248.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "Evaluating on xBD: baseline: 100%|██████████| 63/63 [10:24<00:00,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD baseline]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993386    0.900725\n",
      "std    142.971455    0.029563    0.204571\n",
      "min      0.000000    0.607132    0.299647\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_half: 100%|██████████| 63/63 [10:28<00:00,  9.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD lic_half]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.932707    0.559485\n",
      "std    142.971455    0.086282    0.242646\n",
      "min      0.000000    0.485046    0.181788\n",
      "25%    124.500000    0.897469    0.430180\n",
      "50%    246.000000    0.968918    0.487396\n",
      "75%    370.500000    0.998138    0.511667\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_full: 100%|██████████| 63/63 [10:22<00:00,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD lic_full]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.904372    0.486507\n",
      "std    142.971455    0.119239    0.250159\n",
      "min      0.000000    0.365463    0.121821\n",
      "25%    124.500000    0.852379    0.305191\n",
      "50%    246.000000    0.952408    0.455956\n",
      "75%    370.500000    0.995987    0.498734\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#goal: Test how the model behaves differently once it's fine tuned (are there any patterns?) also test how well the different models perform on LIC_test. LIC_test does have a damage mask as somewhat ground truth so use that.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ Dataset =============\n",
    "class LICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_iou(pred, target, num_classes=4):\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "    axs[0].set_title(\"Pre-disaster\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                iou = compute_iou(y_pred[i], y[i])\n",
    "                results.append({\"filename\": fname_i, \"accuracy\": acc, \"iou\": iou})\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "# ============ Load and Run ============\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"lic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"lic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# Dataset\n",
    "test_dir = os.path.join(base_dir, \"LIC_pseudo\", \"test\")\n",
    "test_dataset = LICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3\n",
    "#bright_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"bright\")\n",
    "#evaluate_and_visualize(baseline, test_loader, device, label=\"baseline\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(half, test_loader, device, label=\"lic_half\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(full, test_loader, device, label=\"lic_full\", save_root=bright_save_root)\n",
    "xbd_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"lic_half\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"lic_full\", save_root=xbd_save_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5b43a0-f111-4991-992c-9dd2a6386721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_26712\\2956508811.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "Evaluating on xBD: baseline: 100%|██████████| 63/63 [10:25<00:00,  9.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD baseline]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993386    0.900725\n",
      "std    142.971455    0.029563    0.204571\n",
      "min      0.000000    0.607132    0.299647\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_half: 100%|██████████| 63/63 [10:27<00:00,  9.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD mic_half]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.928177    0.445440\n",
      "std    142.971455    0.112306    0.294074\n",
      "min      0.000000    0.155121    0.038780\n",
      "25%    124.500000    0.901314    0.242765\n",
      "50%    246.000000    0.979477    0.326355\n",
      "75%    370.500000    0.999702    0.499836\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_full: 100%|██████████| 63/63 [10:21<00:00,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD mic_full]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.917107    0.399179\n",
      "std    142.971455    0.122471    0.292692\n",
      "min      0.000000    0.143768    0.035942\n",
      "25%    124.500000    0.881729    0.225851\n",
      "50%    246.000000    0.975510    0.247620\n",
      "75%    370.500000    0.999222    0.498798\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ Dataset =============\n",
    "class MICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_iou(pred, target, num_classes=4):\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    try:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "        axs[0].set_title(\"Pre-disaster\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "        axs[1].set_title(\"Post SAR\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "        axs[2].set_title(\"Ground Truth\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "        axs[3].set_title(\"Prediction\")\n",
    "        axs[3].axis(\"off\")\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        safe_fname = os.path.basename(fname).replace('.png', '_viz.png').replace(' ', '_')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, safe_fname))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Could not save {fname}: {e}\")\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                iou = compute_iou(y_pred[i], y[i])\n",
    "                results.append({\"filename\": fname_i, \"accuracy\": acc, \"iou\": iou})\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "# ============ Load and Run ============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "mic_base = os.path.join(base_dir, \"MIC_pseudo\")\n",
    "save_root = os.path.join(mic_base, \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"mic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"mic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# MIC Dataset\n",
    "test_dir = os.path.join(mic_base, \"test\")\n",
    "test_dataset = MICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3 models on MIC test set\n",
    "#bright_save_root = os.path.join(mic_base, \"results\", \"bright\")\n",
    "#evaluate_and_visualize(baseline, test_loader, device, label=\"baseline\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(half, test_loader, device, label=\"mic_half\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(full, test_loader, device, label=\"mic_full\", save_root=bright_save_root)\n",
    "\n",
    "# Optional: Evaluate on MIC-style xBD data (if xbd_loader is defined)\n",
    "mic_xbd_save_root = os.path.join(mic_base, \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline\", save_root=mic_xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"mic_half\", save_root=mic_xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"mic_full\", save_root=mic_xbd_save_root)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b4e8b0c-dac4-4fb8-ad19-4c0cd779bdf3",
   "metadata": {},
   "source": [
    "'''import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ Dataset =============\n",
    "class MICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_iou(pred, target, num_classes=4):\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "    axs[0].set_title(\"Pre-disaster\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                iou = compute_iou(y_pred[i], y[i])\n",
    "                results.append({\"filename\": fname_i, \"accuracy\": acc, \"iou\": iou})\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "# ============ Load and Run ============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "save_root = os.path.join(base_dir, \"MIC_pseudo\", \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"mic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"mic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# Dataset\n",
    "test_dir = os.path.join(base_dir, \"MIC_pseudo\", \"test\")\n",
    "test_dataset = MICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3\n",
    "evaluate_and_visualize(baseline, test_loader, device, label=\"baseline\", save_root=save_root)\n",
    "evaluate_and_visualize(half, test_loader, device, label=\"mic_half\", save_root=save_root)\n",
    "evaluate_and_visualize(full, test_loader, device, label=\"mic_full\", save_root=save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddae3b00-4d51-4bfb-8ad9-d5b7d2a53da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_26712\\2520440651.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "Evaluating on xBD: baseline: 100%|██████████| 63/63 [10:26<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD baseline]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993386    0.900725\n",
      "std    142.971455    0.029563    0.204571\n",
      "min      0.000000    0.607132    0.299647\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_half: 100%|██████████| 63/63 [10:29<00:00, 10.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD hic_half]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.898861    0.365926\n",
      "std    142.971455    0.120764    0.136038\n",
      "min      0.000000    0.235657    0.058914\n",
      "25%    124.500000    0.850159    0.275195\n",
      "50%    246.000000    0.945129    0.324808\n",
      "75%    370.500000    0.985489    0.491058\n",
      "max    495.000000    1.000000    1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_full: 100%|██████████| 63/63 [10:24<00:00,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: xBD hic_full]\n",
      "       tile_index    accuracy         iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.682382    0.191288\n",
      "std    142.971455    0.182684    0.065277\n",
      "min      0.000000    0.054825    0.013706\n",
      "25%    124.500000    0.558769    0.149138\n",
      "50%    246.000000    0.689087    0.183025\n",
      "75%    370.500000    0.832489    0.225420\n",
      "max    495.000000    0.995697    0.401630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ Dataset =============\n",
    "class HICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_iou(pred, target, num_classes=4):\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    try:\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "        axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "        axs[0].set_title(\"Pre-disaster\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "        axs[1].set_title(\"Post SAR\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "        axs[2].set_title(\"Ground Truth\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "        axs[3].set_title(\"Prediction\")\n",
    "        axs[3].axis(\"off\")\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        safe_fname = os.path.basename(fname).replace('.png', '_viz.png').replace(' ', '_')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, safe_fname))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Could not save {fname}: {e}\")\n",
    "\n",
    "\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                iou = compute_iou(y_pred[i], y[i])\n",
    "                results.append({\"filename\": fname_i, \"accuracy\": acc, \"iou\": iou})\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "# ============ Load and Run ============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "hic_base = os.path.join(base_dir, \"HIC_pseudo\")\n",
    "save_root = os.path.join(hic_base, \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"hic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"hic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# HIC Dataset\n",
    "test_dir = os.path.join(hic_base, \"test\")\n",
    "test_dataset = HICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3 models on HIC test set\n",
    "#bright_save_root = os.path.join(hic_base, \"results\", \"bright\")\n",
    "#evaluate_and_visualize(baseline, test_loader, device, label=\"baseline\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(half, test_loader, device, label=\"hic_half\", save_root=bright_save_root)\n",
    "#evaluate_and_visualize(full, test_loader, device, label=\"hic_full\", save_root=bright_save_root)\n",
    "\n",
    "# Optional: Evaluate on HIC-style xBD data (if xbd_loader is defined)\n",
    "hic_xbd_save_root = os.path.join(hic_base, \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline\", save_root=hic_xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"hic_half\", save_root=hic_xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"hic_full\", save_root=hic_xbd_save_root)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce36fcc8-4200-4cbc-b01c-8854af77c314",
   "metadata": {},
   "source": [
    "#not js IOU and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a720f6a1-d3f5-44e4-95bc-7eb2deeb1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_40988\\610348004.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "Evaluating baseline_updated: 100%|██████████| 30/30 [01:42<00:00,  3.41s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_40988\\610348004.py:136: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: baseline_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.283409   0.071592        0.283409     0.251412   0.104448\n",
      "std     0.183508   0.047194        0.183508     0.010849   0.051990\n",
      "min     0.051620   0.012905        0.051620     0.250000   0.024543\n",
      "25%     0.162010   0.040503        0.162010     0.250000   0.069696\n",
      "50%     0.242752   0.060688        0.242752     0.250000   0.097667\n",
      "75%     0.361473   0.090368        0.361473     0.250000   0.132749\n",
      "max     0.938889   0.234722        0.938889     0.333333   0.242120\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2834, std=0.1819\n",
      "  precision: mean=0.2834, std=0.1819\n",
      "     recall: mean=1.0000, std=0.0000\n",
      "       dice: mean=0.4139, std=0.1988\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 3:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_half_updated: 100%|██████████| 30/30 [01:41<00:00,  3.40s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_40988\\610348004.py:136: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: lic_half_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.294068   0.094204        0.275408     0.266111   0.142525\n",
      "std     0.188199   0.065494        0.143277     0.036655   0.079478\n",
      "min     0.054626   0.013865        0.057266     0.223151   0.026316\n",
      "25%     0.190033   0.056984        0.175018     0.249620   0.098921\n",
      "50%     0.252228   0.081585        0.232745     0.255830   0.133875\n",
      "75%     0.359863   0.114970        0.373167     0.267138   0.173821\n",
      "max     0.944916   0.336438        0.623771     0.480077   0.440837\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2895, std=0.1939\n",
      "  precision: mean=0.2935, std=0.1949\n",
      "     recall: mean=0.9445, std=0.0645\n",
      "       dice: mean=0.4183, std=0.2076\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0622, std=0.0777\n",
      "  precision: mean=0.2756, std=0.1641\n",
      "     recall: mean=0.0888, std=0.1107\n",
      "       dice: mean=0.1082, std=0.1233\n",
      "Class 3:\n",
      "        iou: mean=0.0194, std=0.0329\n",
      "  precision: mean=0.2614, std=0.2859\n",
      "     recall: mean=0.0230, std=0.0383\n",
      "       dice: mean=0.0362, std=0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_full_updated: 100%|██████████| 30/30 [01:43<00:00,  3.46s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_40988\\610348004.py:136: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY: lic_full_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.301318   0.125132        0.300623     0.305014   0.192537\n",
      "std     0.186440   0.082789        0.134228     0.067107   0.098970\n",
      "min     0.060089   0.017511        0.088565     0.196622   0.033603\n",
      "25%     0.198608   0.079668        0.216208     0.262060   0.135147\n",
      "50%     0.246765   0.105552        0.252275     0.291691   0.176179\n",
      "75%     0.344200   0.133955        0.364894     0.325581   0.218985\n",
      "max     0.960709   0.482361        0.698291     0.614091   0.629608\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2898, std=0.2094\n",
      "  precision: mean=0.3152, std=0.2145\n",
      "     recall: mean=0.7722, std=0.1854\n",
      "       dice: mean=0.4147, std=0.2166\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.1000, std=0.0913\n",
      "  precision: mean=0.2682, std=0.1374\n",
      "     recall: mean=0.1767, std=0.1757\n",
      "       dice: mean=0.1702, std=0.1409\n",
      "Class 3:\n",
      "        iou: mean=0.1026, std=0.0920\n",
      "  precision: mean=0.3211, std=0.2811\n",
      "     recall: mean=0.2607, std=0.2326\n",
      "       dice: mean=0.1745, std=0.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: baseline_updated:   1%|▏         | 77/5598 [13:27<16:04:50, 10.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 170\u001b[0m\n\u001b[0;32m    168\u001b[0m evaluate_and_visualize(full, test_loader, device, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlic_full_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_root\u001b[38;5;241m=\u001b[39mbright_save_root)\n\u001b[0;32m    169\u001b[0m xbd_save_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIC_pseudo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxbd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m evaluate_model_on_xbd(baseline, xbd_loader, device, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_root\u001b[38;5;241m=\u001b[39mxbd_save_root)\n\u001b[0;32m    171\u001b[0m evaluate_model_on_xbd(half, xbd_loader, device, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlic_half_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_root\u001b[38;5;241m=\u001b[39mxbd_save_root)\n\u001b[0;32m    172\u001b[0m evaluate_model_on_xbd(full, xbd_loader, device, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlic_full_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_root\u001b[38;5;241m=\u001b[39mxbd_save_root)\n",
      "Cell \u001b[1;32mIn[1], line 299\u001b[0m, in \u001b[0;36mevaluate_model_on_xbd\u001b[1;34m(model, dataloader, device, label, save_root)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating on xBD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    298\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 299\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m    302\u001b[0m         acc \u001b[38;5;241m=\u001b[39m compute_accuracy(y_pred[j], y[j])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 251\u001b[0m, in \u001b[0;36mUNetOriginal.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    249\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec3(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(d4), e3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    250\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec2(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(d3), e2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 251\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec1(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(d2), e1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(d1)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1154\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv_transpose2d(\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m   1168\u001b[0m     output_padding,\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[0;32m   1171\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============ Dataset =============\n",
    "class LICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_all_metrics(pred, target, num_classes=4):\n",
    "    pred = pred.flatten().cpu().numpy()\n",
    "    target = target.flatten().cpu().numpy()\n",
    "\n",
    "    metrics = {'iou': [], 'precision': [], 'recall': [], 'dice': []}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "\n",
    "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
    "        union = np.logical_or(pred_inds, target_inds).sum()\n",
    "        tp = intersection\n",
    "        fp = np.logical_and(pred_inds, ~target_inds).sum()\n",
    "        fn = np.logical_and(~pred_inds, target_inds).sum()\n",
    "\n",
    "        iou = intersection / union if union > 0 else np.nan\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else np.nan\n",
    "\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['dice'].append(dice)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "    axs[0].set_title(\"Pre-disaster\"); axs[0].axis(\"off\")\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\"); axs[1].axis(\"off\")\n",
    "    axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[2].set_title(\"Ground Truth\"); axs[2].axis(\"off\")\n",
    "    axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[3].set_title(\"Prediction\"); axs[3].axis(\"off\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root, num_classes=4):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    per_class_metrics = {cls: {'iou': [], 'precision': [], 'recall': [], 'dice': []} for cls in range(num_classes)}\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                metrics = compute_all_metrics(y_pred[i], y[i], num_classes=num_classes)\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "                    for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "                        per_class_metrics[cls][metric].append(metrics[metric][cls])\n",
    "\n",
    "                results.append({\n",
    "                    \"filename\": fname_i,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "                })\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save metrics\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print overall summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Per-class summary\n",
    "    print(\"\\n[Per-Class Metrics]\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}:\")\n",
    "        for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "            values = np.array(per_class_metrics[cls][metric])\n",
    "            print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
    "\n",
    "# ============ Load and Run ============\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"lic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"lic_full_finetunedmodel.pt\")\n",
    "\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "test_dir = os.path.join(base_dir, \"LIC_pseudo\", \"test\")\n",
    "test_dataset = LICTestDataset(\n",
    "    os.path.join(test_dir, \"pre\"),\n",
    "    os.path.join(test_dir, \"post\"),\n",
    "    os.path.join(test_dir, \"mask\")\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "bright_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"bright\")\n",
    "evaluate_and_visualize(baseline, test_loader, device, label=\"baseline_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(half, test_loader, device, label=\"lic_half_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(full, test_loader, device, label=\"lic_full_updated\", save_root=bright_save_root)\n",
    "xbd_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline_updated\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"lic_half_updated\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"lic_full_updated\", save_root=xbd_save_root)\n",
    "# Evaluate all models\n",
    "#evaluate_and_visualize(baseline, test_loader, device, label=\"baseline\", save_root=save_root)\n",
    "#evaluate_and_visualize(half, test_loader, device, label=\"lic_half\", save_root=save_root)\n",
    "#evaluate_and_visualize(full, test_loader, device, label=\"lic_full\", save_root=save_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb6775-36c5-46d5-9ef9-0b1acf584ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
