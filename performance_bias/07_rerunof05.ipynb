{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628673f-4bbe-484c-9e98-867ca70a086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================\n",
    "# Imports\n",
    "# ============================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Dataset Classes\n",
    "# ============================\n",
    "class XBDMulticlassDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_pre=None, transform_post=None,\n",
    "                 image_size=(1024, 1024), tile_size=(256, 256), max_images=None):\n",
    "        self.image_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.files = sorted([f for f in os.listdir(self.image_dir) if '_pre_disaster.png' in f])\n",
    "        if max_images:\n",
    "            self.files = self.files[:max_images]\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "        self.image_size = image_size\n",
    "        self.tile_size = tile_size\n",
    "        self.tiles_per_image = (image_size[0] // tile_size[0]) * (image_size[1] // tile_size[1])\n",
    "\n",
    "    def __getitem__(self, idx, return_raw=False):\n",
    "        pre_file = self.files[idx]\n",
    "        post_file = pre_file.replace('pre', 'post')\n",
    "        mask_file = post_file.replace('.png', '_rgb.png')\n",
    "        pre_path = os.path.join(self.image_dir, pre_file)\n",
    "        post_path = os.path.join(self.image_dir, post_file)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "\n",
    "        pre_img_raw = Image.open(pre_path).convert(\"RGB\").resize(self.image_size)\n",
    "        post_img_raw = Image.open(post_path).convert(\"RGB\").resize(self.image_size)\n",
    "        mask_img_raw = Image.open(mask_path).convert(\"RGB\").resize(self.image_size, Image.NEAREST)\n",
    "\n",
    "        def optical_to_sar_like(img):\n",
    "            img = img.convert('L')\n",
    "            img = ImageOps.autocontrast(img, cutoff=2)\n",
    "            return img\n",
    "\n",
    "        def update_mask_multiclass(mask_rgb_img):\n",
    "            mask_np = np.array(mask_rgb_img)\n",
    "            label_mask = np.zeros(mask_np.shape[:2], dtype=np.uint8)\n",
    "            color_to_label = {\n",
    "                (0, 0, 0): 0, (0, 255, 255): 0, (0, 0, 255): 1,\n",
    "                (255, 255, 0): 2, (255, 0, 0): 3, (211, 211, 211): 0\n",
    "            }\n",
    "            for rgb, label in color_to_label.items():\n",
    "                mask = np.all(mask_np == rgb, axis=-1)\n",
    "                label_mask[mask] = label\n",
    "            return label_mask\n",
    "\n",
    "        post_img_sar_raw = optical_to_sar_like(post_img_raw)\n",
    "        if return_raw:\n",
    "            mask_np = update_mask_multiclass(mask_img_raw)\n",
    "            return pre_img_raw, post_img_sar_raw, mask_np\n",
    "\n",
    "        pre_img = self.transform_pre(pre_img_raw) if self.transform_pre else pre_img_raw\n",
    "        post_img_sar = self.transform_post(post_img_sar_raw) if self.transform_post else post_img_sar_raw\n",
    "\n",
    "        input_tensor = torch.cat([pre_img, post_img_sar], dim=0)\n",
    "        mask_tensor = torch.tensor(update_mask_multiclass(mask_img_raw), dtype=torch.long)\n",
    "\n",
    "        return tile_tensor_and_mask(input_tensor, mask_tensor, self.tile_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def get_tile_dataset(self):\n",
    "        return TiledXBDDataset(self)\n",
    "\n",
    "class TiledXBDDataset(Dataset):\n",
    "    def __init__(self, parent_dataset):\n",
    "        self.parent_dataset = parent_dataset\n",
    "        self.tiles_per_image = parent_dataset.tiles_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.tiles_per_image\n",
    "        tile_idx = idx % self.tiles_per_image\n",
    "        tiles_input, tiles_mask = self.parent_dataset[image_idx]\n",
    "        return tiles_input[tile_idx], tiles_mask[tile_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent_dataset) * self.tiles_per_image\n",
    "\n",
    "class FilteredTileDataset(Dataset):\n",
    "    def __init__(self, tiled_dataset, keep_zero_damage_prob=0.1):\n",
    "        self.tiled_dataset = tiled_dataset\n",
    "        self.filtered_indices = []\n",
    "        for idx in range(len(tiled_dataset)):\n",
    "            _, mask = tiled_dataset[idx]\n",
    "            if mask.max() == 0:\n",
    "                if random.random() < keep_zero_damage_prob:\n",
    "                    self.filtered_indices.append(idx)\n",
    "            else:\n",
    "                self.filtered_indices.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tiled_dataset[self.filtered_indices[idx]]\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_majority_class_per_tile(tile_mask):\n",
    "    # Ignore padding or empty masks\n",
    "    return torch.bincount(tile_mask.flatten()).argmax().item()\n",
    "\n",
    "def stratified_tile_sample(dataset, total_samples=500):\n",
    "    class_indices = {0: [], 1: [], 2: [], 3: []}\n",
    "\n",
    "    print(\"Scanning tile classes for stratified sampling...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, mask = dataset[idx]\n",
    "        majority_class = get_majority_class_per_tile(mask)\n",
    "        if majority_class in class_indices:\n",
    "            class_indices[majority_class].append(idx)\n",
    "\n",
    "    # Count all\n",
    "    total_tiles = sum(len(v) for v in class_indices.values())\n",
    "    proportions = {cls: len(v)/total_tiles for cls, v in class_indices.items()}\n",
    "    print(\"Class proportions:\", proportions)\n",
    "\n",
    "    # Sample proportional counts\n",
    "    sampled_indices = []\n",
    "    for cls in range(4):\n",
    "        n_samples = int(proportions[cls] * total_samples)\n",
    "        n_available = len(class_indices[cls])\n",
    "        chosen = random.sample(class_indices[cls], min(n_samples, n_available))\n",
    "        sampled_indices.extend(chosen)\n",
    "\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# ============================\n",
    "# Helper Functions\n",
    "# ============================\n",
    "def tile_tensor_and_mask(input_tensor, mask_tensor, tile_size=(256, 256)):\n",
    "    C, H, W = input_tensor.shape\n",
    "    th, tw = tile_size\n",
    "    tiles_input, tiles_mask = [], []\n",
    "    for i in range(0, H, th):\n",
    "        for j in range(0, W, tw):\n",
    "            tiles_input.append(input_tensor[:, i:i+th, j:j+tw])\n",
    "            tiles_mask.append(mask_tensor[i:i+th, j:j+tw])\n",
    "    return tiles_input, tiles_mask\n",
    "\n",
    "# ============================\n",
    "# Visualization\n",
    "# ============================\n",
    "def unnormalize(img_tensor, mean, std):\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img_tensor\n",
    "\n",
    "def visualize_predictions(inputs, masks, outputs, n=4):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "    cmap = mcolors.ListedColormap(['black', 'blue', 'yellow', 'red'])\n",
    "    bounds = [0, 1, 2, 3, 4]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    for i in range(min(n, inputs.shape[0])):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        pre_rgb = unnormalize(inputs[i][:3].clone(), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        axs[0].imshow(np.transpose(torch.clamp(pre_rgb, 0, 1).cpu().numpy(), (1, 2, 0)))\n",
    "        axs[0].set_title(\"Pre-disaster RGB\")\n",
    "        axs[0].axis('off')\n",
    "        axs[1].imshow(inputs[i][3].cpu().numpy(), cmap='gray')\n",
    "        axs[1].set_title(\"Post-disaster SAR\")\n",
    "        axs[1].axis('off')\n",
    "        axs[2].imshow(preds[i], cmap=cmap, norm=norm)\n",
    "        axs[2].set_title(\"Predicted\")\n",
    "        axs[2].axis('off')\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        plt.imshow(masks[i], cmap=cmap, norm=norm)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Saved all\")\n",
    "\n",
    "class UNetOriginal(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.final = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "model_path = 'C:/Users/sweta/anaconda_projects/non-trivial/performance_bias/iteration_models/model_epoch7.pt'\n",
    "model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "print(\"model loaded successfully\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transforms (same as training)\n",
    "transform_pre = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_post = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load full xBD dataset\n",
    "xbd_root = r\"C:\\Users\\sweta\\.cache\\kagglehub\\datasets\\qianlanzz\\xbd-dataset\\versions\\1\\xbd\\tier1\"\n",
    "xbd_dataset = XBDMulticlassDataset(\n",
    "    xbd_root, \n",
    "    transform_pre=transform_pre, \n",
    "    transform_post=transform_post,\n",
    "    image_size=(1024, 1024),\n",
    "    tile_size=(256, 256)\n",
    ")\n",
    "\n",
    "# Convert to per-tile dataset for evaluation\n",
    "xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "xbd_loader = DataLoader(xbd_tile_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "def evaluate_model_on_xbd(model, dataloader, device, label, save_root):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    results = []\n",
    "    save_dir = os.path.join(save_root, f\"xbd_{label}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(dataloader, desc=f\"Evaluating on xBD: {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for j in range(x.size(0)):\n",
    "                acc = compute_accuracy(y_pred[j], y[j])\n",
    "                iou = compute_iou(y_pred[j], y[j])\n",
    "                results.append({\n",
    "                    \"tile_index\": i * x.size(0) + j,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"iou\": iou\n",
    "                })\n",
    "\n",
    "    # Save CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_dir, f\"xbd_{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n[SUMMARY: xBD {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "stratified_subset = stratified_tile_sample(xbd_tile_dataset, total_samples=500)\n",
    "xbd_loader = DataLoader(stratified_subset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "#goal: Test how the model behaves differently once it's fine tuned (are there any patterns?) also test how well the different models perform on LIC_test. LIC_test does have a damage mask as somewhat ground truth so use that.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ Dataset =============\n",
    "class LICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "# ============ Metrics ============\n",
    "def compute_iou(pred, target, num_classes=4):\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return np.nanmean(iou_per_class)\n",
    "\n",
    "def compute_dice(pred, target, class_id):\n",
    "    \"\"\"\n",
    "    Compute Dice coefficient for a single class.\n",
    "    pred, target: 1D or 2D torch tensors with class labels.\n",
    "    class_id: int, the class to compute Dice for.\n",
    "    \"\"\"\n",
    "    pred_inds = (pred == class_id)\n",
    "    target_inds = (target == class_id)\n",
    "\n",
    "    intersection = (pred_inds & target_inds).sum().item()\n",
    "    pred_sum = pred_inds.sum().item()\n",
    "    target_sum = target_inds.sum().item()\n",
    "\n",
    "    if pred_sum + target_sum == 0:\n",
    "        return float('nan')  # no samples in pred or target for this class\n",
    "\n",
    "    dice = (2 * intersection) / (pred_sum + target_sum)\n",
    "    return dice\n",
    "\n",
    "def compute_precision(pred, target, class_id):\n",
    "    \"\"\"\n",
    "    Compute Precision for a single class.\n",
    "    \"\"\"\n",
    "    pred_inds = (pred == class_id)\n",
    "    target_inds = (target == class_id)\n",
    "\n",
    "    tp = (pred_inds & target_inds).sum().item()\n",
    "    fp = (pred_inds & (~target_inds)).sum().item()\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        return float('nan')\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "def compute_recall(pred, target, class_id):\n",
    "    \"\"\"\n",
    "    Compute Recall for a single class.\n",
    "    \"\"\"\n",
    "    pred_inds = (pred == class_id)\n",
    "    target_inds = (target == class_id)\n",
    "\n",
    "    tp = (pred_inds & target_inds).sum().item()\n",
    "    fn = ((~pred_inds) & target_inds).sum().item()\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        return float('nan')\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "    \n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "def compute_all_metrics(pred, target, num_classes=4):\n",
    "    pred = pred.flatten().cpu().numpy()\n",
    "    target = target.flatten().cpu().numpy()\n",
    "\n",
    "    metrics = {'iou': [], 'precision': [], 'recall': [], 'dice': []}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "\n",
    "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
    "        union = np.logical_or(pred_inds, target_inds).sum()\n",
    "        tp = intersection\n",
    "        fp = np.logical_and(pred_inds, ~target_inds).sum()\n",
    "        fn = np.logical_and(~pred_inds, target_inds).sum()\n",
    "\n",
    "        iou = intersection / union if union > 0 else np.nan\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else np.nan\n",
    "\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['dice'].append(dice)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "# ============ Visualization ============\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu())\n",
    "    axs[0].set_title(\"Pre-disaster\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(true_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].imshow(pred_mask.cpu(), cmap='tab10', vmin=0, vmax=3)\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "# ============ Evaluation ============\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root, num_classes=4):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    per_class_metrics = {cls: {'iou': [], 'precision': [], 'recall': [], 'dice': []} for cls in range(num_classes)}\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, fname in tqdm(dataloader, desc=f\"Evaluating {label}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                fname_i = fname[i]\n",
    "                acc = compute_accuracy(y_pred[i], y[i])\n",
    "                metrics = compute_all_metrics(y_pred[i], y[i], num_classes=num_classes)\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "                    for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "                        per_class_metrics[cls][metric].append(metrics[metric][cls])\n",
    "\n",
    "                results.append({\n",
    "                    \"filename\": fname_i,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "                })\n",
    "\n",
    "                save_visualization(x[i, :3], x[i, 3:], y_pred[i], y[i], fname_i, vis_dir)\n",
    "\n",
    "    # Save metrics\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(vis_dir, f\"{label}_metrics.csv\"), index=False)\n",
    "\n",
    "    # Print overall summary\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Per-class summary\n",
    "    print(\"\\n[Per-Class Metrics]\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}:\")\n",
    "        for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "            values = np.array(per_class_metrics[cls][metric])\n",
    "            print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
    "    \n",
    "# ============ Load and Run ============\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(path):\n",
    "    model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"lic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"lic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# Dataset\n",
    "test_dir = os.path.join(base_dir, \"LIC_pseudo\", \"test\")\n",
    "test_dataset = LICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3\n",
    "bright_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"bright\")\n",
    "evaluate_and_visualize(baseline, test_loader, device, label=\"baseline_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(half, test_loader, device, label=\"lic_half_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(full, test_loader, device, label=\"lic_full_updated\", save_root=bright_save_root)\n",
    "xbd_save_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline_updated\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"lic_half_updated\", save_root=xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"lic_full_updated\", save_root=xbd_save_root)\n",
    "\n",
    "print(\"LIC COMPLETE\") #we move on to mic\n",
    "# ============ Dataset =============\n",
    "class MICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "mic_base = os.path.join(base_dir, \"MIC_pseudo\")\n",
    "save_root = os.path.join(mic_base, \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"mic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"mic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# MIC Dataset\n",
    "test_dir = os.path.join(mic_base, \"test\")\n",
    "test_dataset = MICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3 models on MIC test set\n",
    "bright_save_root = os.path.join(mic_base, \"results\", \"bright\")\n",
    "evaluate_and_visualize(baseline, test_loader, device, label=\"baseline_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(half, test_loader, device, label=\"mic_half_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(full, test_loader, device, label=\"mic_full_updated\", save_root=bright_save_root)\n",
    "\n",
    "# Optional: Evaluate on MIC-style xBD data (if xbd_loader is defined)\n",
    "mic_xbd_save_root = os.path.join(mic_base, \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline_updated\", save_root=mic_xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"mic_half_updated\", save_root=mic_xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"mic_full_updated\", save_root=mic_xbd_save_root)\n",
    "\n",
    "print(\"DONE WITH MIC\") #move on to hic\n",
    "class HICTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        return x, mask, fname\n",
    "\n",
    "\n",
    "# Paths\n",
    "base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "hic_base = os.path.join(base_dir, \"HIC_pseudo\")\n",
    "save_root = os.path.join(hic_base, \"results\")\n",
    "\n",
    "baseline_path = os.path.join(base_dir, \"iteration_models\", \"model_epoch8.pt\")\n",
    "half_path = os.path.join(base_dir, \"iteration_models\", \"hic_half_finetunedmodel.pt\")\n",
    "full_path = os.path.join(base_dir, \"iteration_models\", \"hic_full_finetunedmodel.pt\")\n",
    "\n",
    "# Load models\n",
    "baseline = load_model(baseline_path)\n",
    "half = load_model(half_path)\n",
    "full = load_model(full_path)\n",
    "\n",
    "# HIC Dataset\n",
    "test_dir = os.path.join(hic_base, \"test\")\n",
    "test_dataset = HICTestDataset(os.path.join(test_dir, \"pre\"),\n",
    "                              os.path.join(test_dir, \"post\"),\n",
    "                              os.path.join(test_dir, \"mask\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Evaluate all 3 models on HIC test set\n",
    "bright_save_root = os.path.join(hic_base, \"results\", \"bright\")\n",
    "evaluate_and_visualize(baseline, test_loader, device, label=\"baseline_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(half, test_loader, device, label=\"hic_half_updated\", save_root=bright_save_root)\n",
    "evaluate_and_visualize(full, test_loader, device, label=\"hic_full_updated\", save_root=bright_save_root)\n",
    "\n",
    "# Optional: Evaluate on HIC-style xBD data (if xbd_loader is defined)\n",
    "hic_xbd_save_root = os.path.join(hic_base, \"results\", \"xbd\")\n",
    "evaluate_model_on_xbd(baseline, xbd_loader, device, label=\"baseline_updated\", save_root=hic_xbd_save_root)\n",
    "evaluate_model_on_xbd(half, xbd_loader, device, label=\"hic_half_updated\", save_root=hic_xbd_save_root)\n",
    "evaluate_model_on_xbd(full, xbd_loader, device, label=\"hic_full_updated\", save_root=hic_xbd_save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8eebf7-4f2f-4054-b1fc-0cf390561986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n",
      "Using device: cpu\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:437: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\lic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\lic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n",
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\mic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\mic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n",
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\hic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\hic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Scanning tile classes for stratified sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44784/44784 [4:19:40<00:00,  2.87it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class proportions: {0: 0.9993971061093248, 1: 0.0, 2: 0.00040192926045016077, 3: 0.00020096463022508038}\n",
      "[LIC Test Dataset] Found 59 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\test\\pre\n",
      "[MIC Test Dataset] Found 542 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\test\\pre\n",
      "[HIC Test Dataset] Found 324 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\test\\pre\n",
      "\n",
      "\n",
      "--- Starting LIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_baseline_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_baseline_updated:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_baseline_updated: 100%|██████████| 30/30 [01:31<00:00,  3.05s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:358: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_baseline_updated\\lic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: lic_baseline_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.283409   0.071592        0.283409     0.251412   0.104448\n",
      "std     0.183508   0.047194        0.183508     0.010849   0.051990\n",
      "min     0.051620   0.012905        0.051620     0.250000   0.024543\n",
      "25%     0.162010   0.040503        0.162010     0.250000   0.069696\n",
      "50%     0.242752   0.060688        0.242752     0.250000   0.097667\n",
      "75%     0.361473   0.090368        0.361473     0.250000   0.132749\n",
      "max     0.938889   0.234722        0.938889     0.333333   0.242120\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2834, std=0.1819\n",
      "  precision: mean=0.2834, std=0.1819\n",
      "     recall: mean=1.0000, std=0.0000\n",
      "       dice: mean=0.4139, std=0.1988\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 3:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "[END EVAL] Model: 'lic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_baseline_updated: 100%|██████████| 63/63 [09:51<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_baseline_updated\\xbd_lic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993480    0.927033\n",
      "std    142.971455    0.028016    0.180515\n",
      "min      0.000000    0.638916    0.301753\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'lic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_half_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_half_finetuned_updated:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_half_finetuned_updated: 100%|██████████| 30/30 [01:27<00:00,  2.90s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:358: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_half_finetuned_updated\\lic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: lic_half_finetuned_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.294068   0.094204        0.275408     0.266111   0.142525\n",
      "std     0.188199   0.065494        0.143277     0.036655   0.079478\n",
      "min     0.054626   0.013865        0.057266     0.223151   0.026316\n",
      "25%     0.190033   0.056984        0.175018     0.249620   0.098921\n",
      "50%     0.252228   0.081585        0.232745     0.255830   0.133875\n",
      "75%     0.359863   0.114970        0.373167     0.267138   0.173821\n",
      "max     0.944916   0.336438        0.623771     0.480077   0.440837\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2895, std=0.1939\n",
      "  precision: mean=0.2935, std=0.1949\n",
      "     recall: mean=0.9445, std=0.0645\n",
      "       dice: mean=0.4183, std=0.2076\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0622, std=0.0777\n",
      "  precision: mean=0.2756, std=0.1641\n",
      "     recall: mean=0.0888, std=0.1107\n",
      "       dice: mean=0.1082, std=0.1233\n",
      "Class 3:\n",
      "        iou: mean=0.0194, std=0.0329\n",
      "  precision: mean=0.2614, std=0.2859\n",
      "     recall: mean=0.0230, std=0.0383\n",
      "       dice: mean=0.0362, std=0.0588\n",
      "[END EVAL] Model: 'lic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_half_finetuned_updated: 100%|██████████| 63/63 [09:40<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_half_finetuned_updated\\xbd_lic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.931178    0.579562\n",
      "std    142.971455    0.087627    0.255206\n",
      "min      0.000000    0.595581    0.198882\n",
      "25%    124.500000    0.884468    0.428734\n",
      "50%    246.000000    0.971649    0.489563\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'lic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_full_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_full_finetuned_updated:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating lic_full_finetuned_updated: 100%|██████████| 30/30 [01:29<00:00,  2.98s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:358: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\lic_full_finetuned_updated\\lic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: lic_full_finetuned_updated]\n",
      "        accuracy   mean_iou  mean_precision  mean_recall  mean_dice\n",
      "count  59.000000  59.000000       59.000000    59.000000  59.000000\n",
      "mean    0.301318   0.125132        0.300623     0.305014   0.192537\n",
      "std     0.186440   0.082789        0.134228     0.067107   0.098970\n",
      "min     0.060089   0.017511        0.088565     0.196622   0.033603\n",
      "25%     0.198608   0.079668        0.216208     0.262060   0.135147\n",
      "50%     0.246765   0.105552        0.252275     0.291691   0.176179\n",
      "75%     0.344200   0.133955        0.364894     0.325581   0.218985\n",
      "max     0.960709   0.482361        0.698291     0.614091   0.629608\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2898, std=0.2094\n",
      "  precision: mean=0.3152, std=0.2145\n",
      "     recall: mean=0.7722, std=0.1854\n",
      "       dice: mean=0.4147, std=0.2166\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.1000, std=0.0913\n",
      "  precision: mean=0.2682, std=0.1374\n",
      "     recall: mean=0.1767, std=0.1757\n",
      "       dice: mean=0.1702, std=0.1409\n",
      "Class 3:\n",
      "        iou: mean=0.1026, std=0.0920\n",
      "  precision: mean=0.3211, std=0.2811\n",
      "     recall: mean=0.2607, std=0.2326\n",
      "       dice: mean=0.1745, std=0.1385\n",
      "[END EVAL] Model: 'lic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_full_finetuned_updated: 100%|██████████| 63/63 [09:42<00:00,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_full_finetuned_updated\\xbd_lic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.903064    0.507916\n",
      "std    142.971455    0.120363    0.271105\n",
      "min      0.000000    0.328400    0.109467\n",
      "25%    124.500000    0.846092    0.309293\n",
      "50%    246.000000    0.957962    0.458839\n",
      "75%    370.500000    0.998451    0.499466\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'lic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "--- Starting MIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_baseline_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_baseline_updated:   0%|          | 0/271 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_baseline_updated: 100%|██████████| 271/271 [13:46<00:00,  3.05s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:358: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_baseline_updated\\mic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: mic_baseline_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  542.000000  542.000000      542.000000   542.000000  542.000000\n",
      "mean     0.226407    0.057182        0.224352     0.251053    0.086865\n",
      "std      0.168111    0.045031        0.167086     0.015584    0.047979\n",
      "min      0.054092    0.013523        0.054092     0.244237    0.025658\n",
      "25%      0.132263    0.033161        0.129459     0.250000    0.058555\n",
      "50%      0.175690    0.044037        0.174339     0.250000    0.074883\n",
      "75%      0.225693    0.056423        0.225220     0.250000    0.092067\n",
      "max      0.939484    0.428413        0.939484     0.500000    0.461447\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2264, std=0.1680\n",
      "  precision: mean=0.2264, std=0.1680\n",
      "     recall: mean=0.9999, std=0.0013\n",
      "       dice: mean=0.3448, std=0.1801\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 3:\n",
      "        iou: mean=0.0000, std=0.0001\n",
      "  precision: mean=0.0226, std=0.0367\n",
      "     recall: mean=0.0000, std=0.0001\n",
      "       dice: mean=0.0000, std=0.0002\n",
      "[END EVAL] Model: 'mic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_baseline_updated: 100%|██████████| 63/63 [09:48<00:00,  9.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_baseline_updated\\xbd_mic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993480    0.927033\n",
      "std    142.971455    0.028016    0.180515\n",
      "min      0.000000    0.638916    0.301753\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'mic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_half_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_half_finetuned_updated:   0%|          | 0/271 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_half_finetuned_updated: 100%|██████████| 271/271 [14:40<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_half_finetuned_updated\\mic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: mic_half_finetuned_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  542.000000  542.000000      542.000000   542.000000  542.000000\n",
      "mean     0.410042    0.217675        0.395924     0.365476    0.327700\n",
      "std      0.156702    0.089139        0.095287     0.088788    0.102814\n",
      "min      0.086456    0.035357        0.048285     0.116818    0.063627\n",
      "25%      0.308361    0.152318        0.329385     0.297070    0.252156\n",
      "50%      0.372131    0.199885        0.379253     0.345783    0.322192\n",
      "75%      0.466103    0.271478        0.454546     0.419043    0.389846\n",
      "max      0.949951    0.539870        0.710470     0.657706    0.665917\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2437, std=0.2596\n",
      "  precision: mean=0.3579, std=0.2547\n",
      "     recall: mean=0.3796, std=0.2964\n",
      "       dice: mean=0.3370, std=0.2661\n",
      "Class 1:\n",
      "        iou: mean=0.1523, std=0.1401\n",
      "  precision: mean=0.3853, std=0.1942\n",
      "     recall: mean=0.2238, std=0.2119\n",
      "       dice: mean=0.2408, std=0.1949\n",
      "Class 2:\n",
      "        iou: mean=0.2096, std=0.0923\n",
      "  precision: mean=0.3176, std=0.1063\n",
      "     recall: mean=0.4290, std=0.2204\n",
      "       dice: mean=0.3371, std=0.1246\n",
      "Class 3:\n",
      "        iou: mean=0.2652, std=0.1559\n",
      "  precision: mean=0.5258, std=0.2120\n",
      "     recall: mean=0.4271, std=0.2622\n",
      "       dice: mean=0.3960, std=0.1907\n",
      "[END EVAL] Model: 'mic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_half_finetuned_updated: 100%|██████████| 63/63 [09:47<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_half_finetuned_updated\\xbd_mic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.930939    0.480239\n",
      "std    142.971455    0.105581    0.322872\n",
      "min      0.000000    0.223526    0.055882\n",
      "25%    124.500000    0.891884    0.243214\n",
      "50%    246.000000    0.982285    0.327428\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'mic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_full_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_full_finetuned_updated:   0%|          | 0/271 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mic_full_finetuned_updated: 100%|██████████| 271/271 [14:43<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\mic_full_finetuned_updated\\mic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: mic_full_finetuned_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  542.000000  542.000000      542.000000   542.000000  542.000000\n",
      "mean     0.424408    0.227494        0.414654     0.384240    0.336345\n",
      "std      0.163272    0.096737        0.097523     0.090900    0.108763\n",
      "min      0.083313    0.028886        0.100769     0.126869    0.053068\n",
      "25%      0.323505    0.161167        0.351422     0.317036    0.261593\n",
      "50%      0.387703    0.206747        0.397895     0.367667    0.325611\n",
      "75%      0.477337    0.270679        0.460838     0.436399    0.400122\n",
      "max      0.974945    0.627752        0.726900     0.866346    0.679618\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.2272, std=0.2761\n",
      "  precision: mean=0.3687, std=0.2738\n",
      "     recall: mean=0.3228, std=0.3119\n",
      "       dice: mean=0.3059, std=0.2892\n",
      "Class 1:\n",
      "        iou: mean=0.1523, std=0.1391\n",
      "  precision: mean=0.4904, std=0.1881\n",
      "     recall: mean=0.2033, std=0.2024\n",
      "       dice: mean=0.2416, std=0.1893\n",
      "Class 2:\n",
      "        iou: mean=0.1842, std=0.0842\n",
      "  precision: mean=0.3095, std=0.1041\n",
      "     recall: mean=0.3612, std=0.2158\n",
      "       dice: mean=0.3028, std=0.1171\n",
      "Class 3:\n",
      "        iou: mean=0.3455, std=0.1532\n",
      "  precision: mean=0.4892, std=0.1913\n",
      "     recall: mean=0.6457, std=0.2617\n",
      "       dice: mean=0.4942, std=0.1718\n",
      "[END EVAL] Model: 'mic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_full_finetuned_updated: 100%|██████████| 63/63 [10:08<00:00,  9.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_full_finetuned_updated\\xbd_mic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.919561    0.439374\n",
      "std    142.971455    0.116963    0.326413\n",
      "min      0.000000    0.162079    0.040520\n",
      "25%    124.500000    0.876343    0.224776\n",
      "50%    246.000000    0.976074    0.247887\n",
      "75%    370.500000    0.999962    0.499981\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'mic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "--- Starting HIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_baseline_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_baseline_updated:   0%|          | 0/162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_baseline_updated: 100%|██████████| 162/162 [09:19<00:00,  3.45s/it]\n",
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\414046600.py:358: RuntimeWarning: Mean of empty slice\n",
      "  print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
      "C:\\Users\\sweta\\anaconda3\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_baseline_updated\\hic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: hic_baseline_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  324.000000  324.000000      324.000000   324.000000  324.000000\n",
      "mean     0.189436    0.048179        0.189436     0.251800    0.074942\n",
      "std      0.157130    0.042315        0.157130     0.012135    0.046880\n",
      "min      0.037201    0.009300        0.037201     0.250000    0.017933\n",
      "25%      0.103283    0.025833        0.103283     0.250000    0.046827\n",
      "50%      0.133553    0.033522        0.133553     0.250000    0.059117\n",
      "75%      0.190083    0.047829        0.190083     0.250000    0.080295\n",
      "max      0.953156    0.293605        0.953156     0.333333    0.312210\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.1894, std=0.1569\n",
      "  precision: mean=0.1894, std=0.1569\n",
      "     recall: mean=1.0000, std=0.0000\n",
      "       dice: mean=0.2958, std=0.1749\n",
      "Class 1:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 2:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "Class 3:\n",
      "        iou: mean=0.0000, std=0.0000\n",
      "  precision: mean=nan, std=nan\n",
      "     recall: mean=0.0000, std=0.0000\n",
      "       dice: mean=0.0000, std=0.0000\n",
      "[END EVAL] Model: 'hic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_baseline_updated: 100%|██████████| 63/63 [10:09<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_baseline_updated\\xbd_hic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.993480    0.927033\n",
      "std    142.971455    0.028016    0.180515\n",
      "min      0.000000    0.638916    0.301753\n",
      "25%    124.500000    1.000000    1.000000\n",
      "50%    246.000000    1.000000    1.000000\n",
      "75%    370.500000    1.000000    1.000000\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'hic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_half_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_half_finetuned_updated:   0%|          | 0/162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_half_finetuned_updated: 100%|██████████| 162/162 [09:18<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_half_finetuned_updated\\hic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: hic_half_finetuned_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  324.000000  324.000000      324.000000   324.000000  324.000000\n",
      "mean     0.311788    0.138416        0.312632     0.288649    0.217823\n",
      "std      0.151756    0.064056        0.116038     0.061405    0.084997\n",
      "min      0.023987    0.014913        0.024635     0.142597    0.028783\n",
      "25%      0.222157    0.097995        0.255706     0.249266    0.163913\n",
      "50%      0.286385    0.133473        0.314959     0.270027    0.220273\n",
      "75%      0.368786    0.172026        0.375450     0.323148    0.276830\n",
      "max      0.949799    0.349564        0.693091     0.498353    0.433355\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.1676, std=0.1990\n",
      "  precision: mean=0.2157, std=0.2062\n",
      "     recall: mean=0.4258, std=0.3123\n",
      "       dice: mean=0.2510, std=0.2145\n",
      "Class 1:\n",
      "        iou: mean=0.1277, std=0.1476\n",
      "  precision: mean=0.3656, std=0.2696\n",
      "     recall: mean=0.2012, std=0.2391\n",
      "       dice: mean=0.1997, std=0.2060\n",
      "Class 2:\n",
      "        iou: mean=0.1863, std=0.1082\n",
      "  precision: mean=0.3124, std=0.1679\n",
      "     recall: mean=0.4085, std=0.2247\n",
      "       dice: mean=0.2998, std=0.1578\n",
      "Class 3:\n",
      "        iou: mean=0.0715, std=0.0922\n",
      "  precision: mean=0.4124, std=0.2761\n",
      "     recall: mean=0.1178, std=0.1702\n",
      "       dice: mean=0.1208, std=0.1469\n",
      "[END EVAL] Model: 'hic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_half_finetuned_updated: 100%|██████████| 63/63 [10:22<00:00,  9.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_half_finetuned_updated\\xbd_hic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.899604    0.370501\n",
      "std    142.971455    0.118905    0.135559\n",
      "min      0.000000    0.306320    0.076580\n",
      "25%    124.500000    0.842590    0.287359\n",
      "50%    246.000000    0.951202    0.326502\n",
      "75%    370.500000    0.986031    0.491871\n",
      "max    495.000000    1.000000    1.000000\n",
      "[END EVAL] xBD Model: 'hic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_full_finetuned_updated' on custom test set.\n",
      "====================\n",
      "Saving visualizations and metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_full_finetuned_updated:   0%|          | 0/162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HICTestDataset] Sample 0 shapes - Input: torch.Size([4, 256, 256]), Mask: torch.Size([256, 256])\n",
      "  Batch 0 shapes -> x: torch.Size([2, 4, 256, 256]), y: torch.Size([2, 256, 256]), y_pred: torch.Size([2, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating hic_full_finetuned_updated: 100%|██████████| 162/162 [09:58<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\hic_full_finetuned_updated\\hic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: hic_full_finetuned_updated]\n",
      "         accuracy    mean_iou  mean_precision  mean_recall   mean_dice\n",
      "count  324.000000  324.000000      324.000000   324.000000  324.000000\n",
      "mean     0.372926    0.149571        0.343983     0.284949    0.226709\n",
      "std      0.148302    0.060775        0.105393     0.060237    0.076342\n",
      "min      0.018570    0.018217        0.044550     0.162084    0.035290\n",
      "25%      0.269974    0.108306        0.273467     0.247382    0.180241\n",
      "50%      0.364014    0.145942        0.335164     0.270537    0.228320\n",
      "75%      0.469788    0.185887        0.422839     0.312967    0.277120\n",
      "max      0.891113    0.335161        0.670525     0.505018    0.415908\n",
      "\n",
      "[Per-Class Metrics]\n",
      "Class 0:\n",
      "        iou: mean=0.1137, std=0.2175\n",
      "  precision: mean=0.2853, std=0.2778\n",
      "     recall: mean=0.1540, std=0.2487\n",
      "       dice: mean=0.1567, std=0.2463\n",
      "Class 1:\n",
      "        iou: mean=0.2961, std=0.1708\n",
      "  precision: mean=0.3579, std=0.2111\n",
      "     recall: mean=0.6784, std=0.1985\n",
      "       dice: mean=0.4295, std=0.2093\n",
      "Class 2:\n",
      "        iou: mean=0.1331, std=0.0867\n",
      "  precision: mean=0.3014, std=0.1611\n",
      "     recall: mean=0.2315, std=0.1559\n",
      "       dice: mean=0.2248, std=0.1340\n",
      "Class 3:\n",
      "        iou: mean=0.0553, std=0.0765\n",
      "  precision: mean=0.4815, std=0.2957\n",
      "     recall: mean=0.0818, std=0.1264\n",
      "       dice: mean=0.0959, std=0.1247\n",
      "[END EVAL] Model: 'hic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_full_finetuned_updated: 100%|██████████| 63/63 [10:38<00:00, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_full_finetuned_updated\\xbd_hic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou\n",
      "count  499.000000  499.000000  499.000000\n",
      "mean   247.136273    0.688153    0.193252\n",
      "std    142.971455    0.174964    0.062264\n",
      "min      0.000000    0.029541    0.007385\n",
      "25%    124.500000    0.561707    0.152918\n",
      "50%    246.000000    0.692703    0.185143\n",
      "75%    370.500000    0.830841    0.226228\n",
      "max    495.000000    0.986847    0.328949\n",
      "[END EVAL] xBD Model: 'hic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "All evaluations complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#gem ver\n",
    "# ============================\n",
    "# Imports\n",
    "# ============================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "# ============================\n",
    "# Model Definition (U-Net)\n",
    "# ============================\n",
    "class UNetOriginal(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.final = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "# ============================\n",
    "# Helper & Metric Functions\n",
    "# ============================\n",
    "def tile_tensor_and_mask(input_tensor, mask_tensor, tile_size=(256, 256)):\n",
    "    C, H, W = input_tensor.shape\n",
    "    th, tw = tile_size\n",
    "    tiles_input, tiles_mask = [], []\n",
    "    for i in range(0, H, th):\n",
    "        for j in range(0, W, tw):\n",
    "            tiles_input.append(input_tensor[:, i:i+th, j:j+tw])\n",
    "            tiles_mask.append(mask_tensor[i:i+th, j:j+tw])\n",
    "    return tiles_input, tiles_mask\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "def compute_all_metrics(pred, target, num_classes=4):\n",
    "    pred_flat = pred.flatten().cpu().numpy()\n",
    "    target_flat = target.flatten().cpu().numpy()\n",
    "    metrics = {'iou': [], 'precision': [], 'recall': [], 'dice': []}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred_flat == cls\n",
    "        target_inds = target_flat == cls\n",
    "        \n",
    "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
    "        union = np.logical_or(pred_inds, target_inds).sum()\n",
    "        tp = intersection\n",
    "        fp = np.logical_and(pred_inds, ~target_inds).sum()\n",
    "        fn = np.logical_and(~pred_inds, target_inds).sum()\n",
    "\n",
    "        iou = tp / union if union > 0 else np.nan\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else np.nan\n",
    "\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['dice'].append(dice)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# ============================\n",
    "# Dataset Classes\n",
    "# ============================\n",
    "class XBDMulticlassDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_pre=None, transform_post=None,\n",
    "                 image_size=(1024, 1024), tile_size=(256, 256), max_images=None):\n",
    "        self.image_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.files = sorted([f for f in os.listdir(self.image_dir) if '_pre_disaster.png' in f])\n",
    "        if max_images:\n",
    "            self.files = self.files[:max_images]\n",
    "        # DEBUG: Confirm number of images found\n",
    "        print(f\"[XBDDataset] Found {len(self.files)} pre-disaster images in {self.image_dir}\")\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "        self.image_size = image_size\n",
    "        self.tile_size = tile_size\n",
    "        self.tiles_per_image = (image_size[0] // tile_size[0]) * (image_size[1] // tile_size[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pre_file = self.files[idx]\n",
    "        post_file = pre_file.replace('pre', 'post')\n",
    "        mask_file = post_file.replace('.png', '_rgb.png')\n",
    "        pre_path = os.path.join(self.image_dir, pre_file)\n",
    "        post_path = os.path.join(self.image_dir, post_file)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "\n",
    "        pre_img_raw = Image.open(pre_path).convert(\"RGB\").resize(self.image_size)\n",
    "        post_img_raw = Image.open(post_path).convert(\"RGB\").resize(self.image_size)\n",
    "        mask_img_raw = Image.open(mask_path).convert(\"RGB\").resize(self.image_size, Image.NEAREST)\n",
    "\n",
    "        def optical_to_sar_like(img):\n",
    "            img = img.convert('L')\n",
    "            img = ImageOps.autocontrast(img, cutoff=2)\n",
    "            return img\n",
    "\n",
    "        def update_mask_multiclass(mask_rgb_img):\n",
    "            mask_np = np.array(mask_rgb_img)\n",
    "            label_mask = np.zeros(mask_np.shape[:2], dtype=np.uint8)\n",
    "            color_to_label = {\n",
    "                (0, 0, 0): 0, (0, 255, 255): 0, (0, 0, 255): 1,\n",
    "                (255, 255, 0): 2, (255, 0, 0): 3, (211, 211, 211): 0\n",
    "            }\n",
    "            for rgb, label in color_to_label.items():\n",
    "                mask = np.all(mask_np == rgb, axis=-1)\n",
    "                label_mask[mask] = label\n",
    "            return label_mask\n",
    "\n",
    "        post_img_sar_raw = optical_to_sar_like(post_img_raw)\n",
    "        pre_img = self.transform_pre(pre_img_raw) if self.transform_pre else pre_img_raw\n",
    "        post_img_sar = self.transform_post(post_img_sar_raw) if self.transform_post else post_img_sar_raw\n",
    "\n",
    "        input_tensor = torch.cat([pre_img, post_img_sar], dim=0)\n",
    "        mask_tensor = torch.tensor(update_mask_multiclass(mask_img_raw), dtype=torch.long)\n",
    "\n",
    "        return tile_tensor_and_mask(input_tensor, mask_tensor, self.tile_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def get_tile_dataset(self):\n",
    "        return TiledXBDDataset(self)\n",
    "\n",
    "class TiledXBDDataset(Dataset):\n",
    "    def __init__(self, parent_dataset):\n",
    "        self.parent_dataset = parent_dataset\n",
    "        self.tiles_per_image = parent_dataset.tiles_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.tiles_per_image\n",
    "        tile_idx = idx % self.tiles_per_image\n",
    "        tiles_input, tiles_mask = self.parent_dataset[image_idx]\n",
    "        return tiles_input[tile_idx], tiles_mask[tile_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent_dataset) * self.tiles_per_image\n",
    "\n",
    "# Note: The three test datasets are identical. They could be refactored into one class.\n",
    "class BaseTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir, dataset_name=\"Test\"):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        # DEBUG: Confirm number of images found\n",
    "        print(f\"[{dataset_name} Dataset] Found {len(self.files)} images in {pre_dir}\")\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        \n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        \n",
    "        # DEBUG: Check tensor shapes for the first item\n",
    "        if idx == 0:\n",
    "            print(f\"[{self.__class__.__name__}] Sample 0 shapes - Input: {x.shape}, Mask: {mask.shape}\")\n",
    "            \n",
    "        return x, mask, fname\n",
    "\n",
    "class LICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"LIC Test\")\n",
    "\n",
    "class MICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"MIC Test\")\n",
    "\n",
    "class HICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"HIC Test\")\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_majority_class_per_tile(tile_mask):\n",
    "    # Ignore padding or empty masks\n",
    "    return torch.bincount(tile_mask.flatten()).argmax().item()\n",
    "\n",
    "def stratified_tile_sample(dataset, total_samples=500):\n",
    "    class_indices = {0: [], 1: [], 2: [], 3: []}\n",
    "\n",
    "    print(\"Scanning tile classes for stratified sampling...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, mask = dataset[idx]\n",
    "        majority_class = get_majority_class_per_tile(mask)\n",
    "        if majority_class in class_indices:\n",
    "            class_indices[majority_class].append(idx)\n",
    "\n",
    "    # Count all\n",
    "    total_tiles = sum(len(v) for v in class_indices.values())\n",
    "    proportions = {cls: len(v)/total_tiles for cls, v in class_indices.items()}\n",
    "    print(\"Class proportions:\", proportions)\n",
    "\n",
    "    # Sample proportional counts\n",
    "    sampled_indices = []\n",
    "    for cls in range(4):\n",
    "        n_samples = int(proportions[cls] * total_samples)\n",
    "        n_available = len(class_indices[cls])\n",
    "        chosen = random.sample(class_indices[cls], min(n_samples, n_available))\n",
    "        sampled_indices.extend(chosen)\n",
    "\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# ============================\n",
    "# Evaluation & Visualization\n",
    "# ============================\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    cmap = mcolors.ListedColormap(['black', 'blue', 'yellow', 'red'])\n",
    "    norm = mcolors.BoundaryNorm(np.arange(-0.5, 4, 1), cmap.N)\n",
    "\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu().numpy())\n",
    "    axs[0].set_title(\"Pre-disaster\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu().numpy(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(true_mask.cpu().numpy(), cmap=cmap, norm=norm)\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].imshow(pred_mask.cpu().numpy(), cmap=cmap, norm=norm)\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root, num_classes=4):\n",
    "    print(f\"\\n{'='*20}\\n[START EVAL] Model: '{label}' on custom test set.\\n{'='*20}\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    per_class_metrics = {cls: {m: [] for m in ['iou', 'precision', 'recall', 'dice']} for cls in range(num_classes)}\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    # DEBUG: Show where results will be saved\n",
    "    print(f\"Saving visualizations and metrics to: {vis_dir}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, fname) in enumerate(tqdm(dataloader, desc=f\"Evaluating {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "            \n",
    "            # DEBUG: Check tensor shapes inside the loop (only for the first batch)\n",
    "            if i == 0:\n",
    "                print(f\"  Batch 0 shapes -> x: {x.shape}, y: {y.shape}, y_pred: {y_pred.shape}\")\n",
    "\n",
    "            for j in range(len(y)):\n",
    "                fname_i = fname[j]\n",
    "                pred_j, y_j = y_pred[j], y[j]\n",
    "                \n",
    "                acc = compute_accuracy(pred_j, y_j)\n",
    "                metrics = compute_all_metrics(pred_j, y_j, num_classes=num_classes)\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "                    for metric_name in per_class_metrics[cls]:\n",
    "                        per_class_metrics[cls][metric_name].append(metrics[metric_name][cls])\n",
    "\n",
    "                results.append({\n",
    "                    \"filename\": fname_i, \"accuracy\": acc,\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "                })\n",
    "                save_visualization(x[j, :3], x[j, 3:], pred_j, y_j, fname_i, vis_dir)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(vis_dir, f\"{label}_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    # DEBUG: Confirm CSV saved\n",
    "    print(f\"Metrics saved to {csv_path}\")\n",
    "\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n[Per-Class Metrics]\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}:\")\n",
    "        for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "            values = np.array(per_class_metrics[cls][metric])\n",
    "            print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
    "    print(f\"[END EVAL] Model: '{label}'.\\n{'='*20}\")\n",
    "\n",
    "def evaluate_model_on_xbd(model, dataloader, device, label, save_root):\n",
    "    print(f\"\\n{'='*20}\\n[START EVAL] Model: '{label}' on xBD tiles.\\n{'='*20}\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    results = []\n",
    "    save_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # DEBUG: Show where results will be saved\n",
    "    print(f\"Saving xBD metrics to: {save_dir}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(dataloader, desc=f\"Evaluating on xBD: {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for j in range(x.size(0)):\n",
    "                pred_j, y_j = y_pred[j], y[j]\n",
    "                metrics = compute_all_metrics(pred_j, y_j, num_classes=4)\n",
    "                results.append({\n",
    "                    \"tile_index\": i * x.size(0) + j,\n",
    "                    \"accuracy\": compute_accuracy(pred_j, y_j),\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(save_dir, f\"xbd_{label}_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    # DEBUG: Confirm CSV saved\n",
    "    print(f\"xBD metrics saved to {csv_path}\")\n",
    "\n",
    "    print(f\"\\n[SUMMARY: xBD {label}]\")\n",
    "    print(df.describe())\n",
    "    print(f\"[END EVAL] xBD Model: '{label}'.\\n{'='*20}\")\n",
    "\n",
    "# ============================\n",
    "# Main Execution\n",
    "# ============================\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Step 1: Define Paths ---\n",
    "    base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "    \n",
    "    # Model paths\n",
    "    iteration_models_dir = os.path.join(base_dir, \"iteration_models\")\n",
    "    baseline_path = os.path.join(iteration_models_dir, \"model_epoch8.pt\")\n",
    "    \n",
    "    lic_half_path = os.path.join(iteration_models_dir, \"lic_half_finetunedmodel.pt\")\n",
    "    lic_full_path = os.path.join(iteration_models_dir, \"lic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    mic_half_path = os.path.join(iteration_models_dir, \"mic_half_finetunedmodel.pt\")\n",
    "    mic_full_path = os.path.join(iteration_models_dir, \"mic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    hic_half_path = os.path.join(iteration_models_dir, \"hic_half_finetunedmodel.pt\")\n",
    "    hic_full_path = os.path.join(iteration_models_dir, \"hic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    # Data paths\n",
    "    xbd_root = r\"C:\\Users\\sweta\\.cache\\kagglehub\\datasets\\qianlanzz\\xbd-dataset\\versions\\1\\xbd\\tier1\"\n",
    "    lic_test_dir = os.path.join(base_dir, \"LIC_pseudo\", \"test\")\n",
    "    mic_test_dir = os.path.join(base_dir, \"MIC_pseudo\", \"test\")\n",
    "    hic_test_dir = os.path.join(base_dir, \"HIC_pseudo\", \"test\")\n",
    "\n",
    "    # Results paths\n",
    "    lic_results_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\")\n",
    "    mic_results_root = os.path.join(base_dir, \"MIC_pseudo\", \"results\")\n",
    "    hic_results_root = os.path.join(base_dir, \"HIC_pseudo\", \"results\")\n",
    "\n",
    "    # --- Step 2: Load Models ---\n",
    "    def load_model_from_path(path):\n",
    "        print(f\"Loading model from: {path}\")\n",
    "        model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  [ERROR] Model path does not exist: {path}\")\n",
    "            return None\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        # Handle both dict and checkpoint objects\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            # DEBUG\n",
    "            print(\"  Loading from a checkpoint object...\")\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # DEBUG\n",
    "            print(\"  Loading from a raw state dictionary...\")\n",
    "            model.load_state_dict(checkpoint)\n",
    "        return model\n",
    "\n",
    "    models_to_test = {\n",
    "        \"LIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(lic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(lic_full_path),\n",
    "        },\n",
    "        \"MIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(mic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(mic_full_path),\n",
    "        },\n",
    "        \"HIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(hic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(hic_full_path),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # --- Step 3: Prepare DataLoaders ---\n",
    "    # Create xBD loader (used for all evaluations)\n",
    "    transform_pre = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    transform_post = T.Compose([T.ToTensor(), T.Normalize(mean=[0.5], std=[0.5])])\n",
    "    from torch.utils.data import Subset\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "    stratified_subset = stratified_tile_sample(xbd_tile_dataset, total_samples=500)\n",
    "    xbd_loader = DataLoader(stratified_subset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "    # Create LIC, MIC, HIC loaders\n",
    "    lic_test_dataset = LICTestDataset(os.path.join(lic_test_dir, \"pre\"), os.path.join(lic_test_dir, \"post\"), os.path.join(lic_test_dir, \"mask\"))\n",
    "    lic_loader = DataLoader(lic_test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    mic_test_dataset = MICTestDataset(os.path.join(mic_test_dir, \"pre\"), os.path.join(mic_test_dir, \"post\"), os.path.join(mic_test_dir, \"mask\"))\n",
    "    mic_loader = DataLoader(mic_test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    hic_test_dataset = HICTestDataset(os.path.join(hic_test_dir, \"pre\"), os.path.join(hic_test_dir, \"post\"), os.path.join(hic_test_dir, \"mask\"))\n",
    "    hic_loader = DataLoader(hic_test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    # --- Step 4: Run Evaluations ---\n",
    "    \n",
    "    # Evaluate on LIC\n",
    "    print(\"\\n\\n--- Starting LIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"LIC\"].items():\n",
    "        if model:\n",
    "            evaluate_and_visualize(model, lic_loader, device, label=f\"lic_{name}_updated\", save_root=lic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"lic_{name}_updated\", save_root=os.path.join(lic_results_root, \"xbd_eval\"))\n",
    "\n",
    "    # Evaluate on MIC\n",
    "    print(\"\\n\\n--- Starting MIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"MIC\"].items():\n",
    "        if model:\n",
    "            evaluate_and_visualize(model, mic_loader, device, label=f\"mic_{name}_updated\", save_root=mic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"mic_{name}_updated\", save_root=os.path.join(mic_results_root, \"xbd_eval\"))\n",
    "            \n",
    "    # Evaluate on HIC\n",
    "    print(\"\\n\\n--- Starting HIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"HIC\"].items():\n",
    "        if model:\n",
    "            evaluate_and_visualize(model, hic_loader, device, label=f\"hic_{name}_updated\", save_root=hic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"hic_{name}_updated\", save_root=os.path.join(hic_results_root, \"xbd_eval\"))\n",
    "\n",
    "    print(\"\\n\\nAll evaluations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57b315b-3bab-403e-afcf-be3a2d01579e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [Summary for LIC] ===\n",
      "[INFO] Root directory: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\n",
      "  → [baseline] mean_iou: 59 valid entries\n",
      "  → [baseline] mean_precision: 59 valid entries\n",
      "  → [baseline] mean_recall: 59 valid entries\n",
      "  → [baseline] mean_dice: 59 valid entries\n",
      "  → [half_finetuned] mean_iou: 59 valid entries\n",
      "  → [half_finetuned] mean_precision: 59 valid entries\n",
      "  → [half_finetuned] mean_recall: 59 valid entries\n",
      "  → [half_finetuned] mean_dice: 59 valid entries\n",
      "  → [full_finetuned] mean_iou: 59 valid entries\n",
      "  → [full_finetuned] mean_precision: 59 valid entries\n",
      "  → [full_finetuned] mean_recall: 59 valid entries\n",
      "  → [full_finetuned] mean_dice: 59 valid entries\n",
      "\n",
      "-- BASELINE --\n",
      "     mean_iou: mean=0.0716, median=0.0607, 95% CI=(0.0593, 0.0839)\n",
      "mean_precision: mean=0.2834, median=0.2428, 95% CI=(0.2356, 0.3312)\n",
      "  mean_recall: mean=0.2514, median=0.2500, 95% CI=(0.2486, 0.2542)\n",
      "    mean_dice: mean=0.1044, median=0.0977, 95% CI=(0.0909, 0.1180)\n",
      "\n",
      "-- HALF_FINETUNED --\n",
      "     mean_iou: mean=0.0942, median=0.0816, 95% CI=(0.0771, 0.1113)\n",
      "mean_precision: mean=0.2754, median=0.2327, 95% CI=(0.2381, 0.3127)\n",
      "  mean_recall: mean=0.2661, median=0.2558, 95% CI=(0.2566, 0.2757)\n",
      "    mean_dice: mean=0.1425, median=0.1339, 95% CI=(0.1218, 0.1632)\n",
      "\n",
      "-- FULL_FINETUNED --\n",
      "     mean_iou: mean=0.1251, median=0.1056, 95% CI=(0.1036, 0.1467)\n",
      "mean_precision: mean=0.3006, median=0.2523, 95% CI=(0.2656, 0.3356)\n",
      "  mean_recall: mean=0.3050, median=0.2917, 95% CI=(0.2875, 0.3225)\n",
      "    mean_dice: mean=0.1925, median=0.1762, 95% CI=(0.1667, 0.2183)\n",
      "\n",
      "=== [Summary for MIC] ===\n",
      "[INFO] Root directory: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\n",
      "  → [baseline] mean_iou: 542 valid entries\n",
      "  → [baseline] mean_precision: 542 valid entries\n",
      "  → [baseline] mean_recall: 542 valid entries\n",
      "  → [baseline] mean_dice: 542 valid entries\n",
      "  → [half_finetuned] mean_iou: 542 valid entries\n",
      "  → [half_finetuned] mean_precision: 542 valid entries\n",
      "  → [half_finetuned] mean_recall: 542 valid entries\n",
      "  → [half_finetuned] mean_dice: 542 valid entries\n",
      "  → [full_finetuned] mean_iou: 542 valid entries\n",
      "  → [full_finetuned] mean_precision: 542 valid entries\n",
      "  → [full_finetuned] mean_recall: 542 valid entries\n",
      "  → [full_finetuned] mean_dice: 542 valid entries\n",
      "\n",
      "-- BASELINE --\n",
      "     mean_iou: mean=0.0572, median=0.0440, 95% CI=(0.0534, 0.0610)\n",
      "mean_precision: mean=0.2244, median=0.1743, 95% CI=(0.2103, 0.2385)\n",
      "  mean_recall: mean=0.2511, median=0.2500, 95% CI=(0.2497, 0.2524)\n",
      "    mean_dice: mean=0.0869, median=0.0749, 95% CI=(0.0828, 0.0909)\n",
      "\n",
      "-- HALF_FINETUNED --\n",
      "     mean_iou: mean=0.2177, median=0.1999, 95% CI=(0.2102, 0.2252)\n",
      "mean_precision: mean=0.3959, median=0.3793, 95% CI=(0.3879, 0.4040)\n",
      "  mean_recall: mean=0.3655, median=0.3458, 95% CI=(0.3580, 0.3730)\n",
      "    mean_dice: mean=0.3277, median=0.3222, 95% CI=(0.3190, 0.3364)\n",
      "\n",
      "-- FULL_FINETUNED --\n",
      "     mean_iou: mean=0.2275, median=0.2067, 95% CI=(0.2193, 0.2357)\n",
      "mean_precision: mean=0.4147, median=0.3979, 95% CI=(0.4064, 0.4229)\n",
      "  mean_recall: mean=0.3842, median=0.3677, 95% CI=(0.3766, 0.3919)\n",
      "    mean_dice: mean=0.3363, median=0.3256, 95% CI=(0.3272, 0.3455)\n",
      "\n",
      "=== [Summary for HIC] ===\n",
      "[INFO] Root directory: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\n",
      "  → [baseline] mean_iou: 324 valid entries\n",
      "  → [baseline] mean_precision: 324 valid entries\n",
      "  → [baseline] mean_recall: 324 valid entries\n",
      "  → [baseline] mean_dice: 324 valid entries\n",
      "  → [half_finetuned] mean_iou: 324 valid entries\n",
      "  → [half_finetuned] mean_precision: 324 valid entries\n",
      "  → [half_finetuned] mean_recall: 324 valid entries\n",
      "  → [half_finetuned] mean_dice: 324 valid entries\n",
      "  → [full_finetuned] mean_iou: 324 valid entries\n",
      "  → [full_finetuned] mean_precision: 324 valid entries\n",
      "  → [full_finetuned] mean_recall: 324 valid entries\n",
      "  → [full_finetuned] mean_dice: 324 valid entries\n",
      "\n",
      "-- BASELINE --\n",
      "     mean_iou: mean=0.0482, median=0.0335, 95% CI=(0.0436, 0.0528)\n",
      "mean_precision: mean=0.1894, median=0.1336, 95% CI=(0.1723, 0.2066)\n",
      "  mean_recall: mean=0.2518, median=0.2500, 95% CI=(0.2505, 0.2531)\n",
      "    mean_dice: mean=0.0749, median=0.0591, 95% CI=(0.0698, 0.0801)\n",
      "\n",
      "-- HALF_FINETUNED --\n",
      "     mean_iou: mean=0.1384, median=0.1335, 95% CI=(0.1314, 0.1454)\n",
      "mean_precision: mean=0.3126, median=0.3150, 95% CI=(0.2999, 0.3253)\n",
      "  mean_recall: mean=0.2886, median=0.2700, 95% CI=(0.2819, 0.2954)\n",
      "    mean_dice: mean=0.2178, median=0.2203, 95% CI=(0.2085, 0.2271)\n",
      "\n",
      "-- FULL_FINETUNED --\n",
      "     mean_iou: mean=0.1496, median=0.1459, 95% CI=(0.1429, 0.1562)\n",
      "mean_precision: mean=0.3440, median=0.3352, 95% CI=(0.3325, 0.3555)\n",
      "  mean_recall: mean=0.2849, median=0.2705, 95% CI=(0.2784, 0.2915)\n",
      "    mean_dice: mean=0.2267, median=0.2283, 95% CI=(0.2184, 0.2351)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "def compute_summary_stats(values, metric_name=\"\", model_label=\"\"):\n",
    "    values = np.array(values)\n",
    "    values = values[~np.isnan(values)]  # remove NaNs\n",
    "\n",
    "    print(f\"  → [{model_label}] {metric_name}: {len(values)} valid entries\")\n",
    "\n",
    "    if len(values) == 0:\n",
    "        print(f\"    [WARNING] No valid entries for {metric_name} in {model_label}\")\n",
    "        return {\n",
    "            \"mean\": np.nan,\n",
    "            \"median\": np.nan,\n",
    "            \"ci95_low\": np.nan,\n",
    "            \"ci95_high\": np.nan\n",
    "        }\n",
    "\n",
    "    mean = np.mean(values)\n",
    "    median = np.median(values)\n",
    "    ci95 = st.t.interval(\n",
    "        confidence=0.95,\n",
    "        df=len(values) - 1,\n",
    "        loc=mean,\n",
    "        scale=st.sem(values) if len(values) > 1 else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"median\": median,\n",
    "        \"ci95_low\": ci95[0],\n",
    "        \"ci95_high\": ci95[1]\n",
    "    }\n",
    "\n",
    "def summarize_bright_metrics(results_root, region_label):\n",
    "    print(f\"\\n=== [Summary for {region_label}] ===\")\n",
    "    print(f\"[INFO] Root directory: {results_root}\")\n",
    "    \n",
    "    models = [\"baseline\", \"half_finetuned\", \"full_finetuned\"]\n",
    "    metrics = [\"mean_iou\", \"mean_precision\", \"mean_recall\", \"mean_dice\"]\n",
    "    summary = {}\n",
    "\n",
    "    for model in models:\n",
    "        label = f\"{region_label.lower()}_{model}_updated\"\n",
    "        csv_path = os.path.join(results_root, label, f\"{label}_metrics.csv\")\n",
    "        \n",
    "        #print(f\"\\n[DEBUG] Looking for: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            #print(f\"  [!] Missing file: {csv_path}\")\n",
    "            continue\n",
    "\n",
    "        #print(f\"  [+] Loading CSV for model: {model.upper()}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        #print(f\"    → CSV loaded. Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "\n",
    "        model_stats = {}\n",
    "        for metric in metrics:\n",
    "            if metric not in df.columns:\n",
    "                #print(f\"    [WARNING] Metric '{metric}' not found in CSV!\")\n",
    "                continue\n",
    "            stats = compute_summary_stats(df[metric], metric_name=metric, model_label=model)\n",
    "            model_stats[metric] = stats\n",
    "\n",
    "        summary[model] = model_stats\n",
    "\n",
    "    # Pretty print results\n",
    "    for model, stats in summary.items():\n",
    "        print(f\"\\n-- {model.upper()} --\")\n",
    "        for metric, values in stats.items():\n",
    "            print(f\"{metric:>13}: mean={values['mean']:.4f}, median={values['median']:.4f}, 95% CI=({values['ci95_low']:.4f}, {values['ci95_high']:.4f})\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# After all evaluations complete\n",
    "lic_summary = summarize_bright_metrics(lic_results_root, \"LIC\")\n",
    "mic_summary = summarize_bright_metrics(mic_results_root, \"MIC\")\n",
    "hic_summary = summarize_bright_metrics(hic_results_root, \"HIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab46a301-097f-4fb1-a9b4-c929bec94783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n",
      "Using device: cpu\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta\\AppData\\Local\\Temp\\ipykernel_42256\\2115860600.py:441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\lic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\lic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n",
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\mic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\mic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\model_epoch8.pt\n",
      "  Loading from a checkpoint object...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\hic_half_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Loading model from: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\iteration_models\\hic_full_finetunedmodel.pt\n",
      "  Loading from a raw state dictionary...\n",
      "Scanning tile classes for stratified sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44784/44784 [4:00:34<00:00,  3.10it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class proportions: {0: 0.9993971061093248, 1: 0.0, 2: 0.00040192926045016077, 3: 0.00020096463022508038}\n",
      "[LIC Test Dataset] Found 59 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\test\\pre\n",
      "[MIC Test Dataset] Found 542 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\test\\pre\n",
      "[HIC Test Dataset] Found 324 images in C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\test\\pre\n",
      "\n",
      "\n",
      "--- Starting LIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_baseline_updated: 100%|██████████| 63/63 [09:52<00:00,  9.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_baseline_updated\\xbd_lic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.994949    0.915736        0.985393     0.923734   \n",
      "std    142.971455    0.020293    0.192093        0.069192     0.181397   \n",
      "min      0.000000    0.785706    0.284500        0.454786     0.333333   \n",
      "25%    124.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "50%    246.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.918539  \n",
      "std      0.188181  \n",
      "min      0.306987  \n",
      "25%      1.000000  \n",
      "50%      1.000000  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'lic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_half_finetuned_updated: 100%|██████████| 63/63 [09:48<00:00,  9.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_half_finetuned_updated\\xbd_lic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.932510    0.578811        0.625101     0.877617   \n",
      "std    142.971455    0.092291    0.257956        0.230006     0.179626   \n",
      "min      0.000000    0.462814    0.213852        0.326088     0.318739   \n",
      "25%    124.500000    0.906540    0.427540        0.500000     0.837217   \n",
      "50%    246.000000    0.972473    0.488045        0.500000     0.966034   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.595327  \n",
      "std      0.248065  \n",
      "min      0.260548  \n",
      "25%      0.463629  \n",
      "50%      0.494874  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'lic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'lic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: lic_full_finetuned_updated: 100%|██████████| 63/63 [09:51<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\LIC_pseudo\\results\\xbd_eval\\lic_full_finetuned_updated\\xbd_lic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD lic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.904028    0.503843        0.553856     0.856113   \n",
      "std    142.971455    0.127911    0.266966        0.240428     0.190016   \n",
      "min      0.000000    0.307449    0.102483        0.318583     0.286896   \n",
      "25%    124.500000    0.856323    0.306836        0.333333     0.782639   \n",
      "50%    246.000000    0.960052    0.464058        0.500000     0.948685   \n",
      "75%    370.500000    0.998634    0.499847        0.547853     0.998596   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.522289  \n",
      "std      0.256454  \n",
      "min      0.156768  \n",
      "25%      0.322498  \n",
      "50%      0.483081  \n",
      "75%      0.499956  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'lic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "--- Starting MIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_baseline_updated: 100%|██████████| 63/63 [09:18<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_baseline_updated\\xbd_mic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.994949    0.915736        0.985393     0.923734   \n",
      "std    142.971455    0.020293    0.192093        0.069192     0.181397   \n",
      "min      0.000000    0.785706    0.284500        0.454786     0.333333   \n",
      "25%    124.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "50%    246.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.918539  \n",
      "std      0.188181  \n",
      "min      0.306987  \n",
      "25%      1.000000  \n",
      "50%      1.000000  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'mic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_half_finetuned_updated: 100%|██████████| 63/63 [09:15<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_half_finetuned_updated\\xbd_mic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.929429    0.481271        0.509299     0.878730   \n",
      "std    142.971455    0.120692    0.323391        0.310860     0.185297   \n",
      "min      0.000000    0.167206    0.041801        0.235047     0.167206   \n",
      "25%    124.500000    0.908470    0.242768        0.250000     0.840748   \n",
      "50%    246.000000    0.984909    0.328064        0.333333     0.981186   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.491852  \n",
      "std      0.316338  \n",
      "min      0.071627  \n",
      "25%      0.247769  \n",
      "50%      0.331098  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'mic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'mic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: mic_full_finetuned_updated: 100%|██████████| 63/63 [09:15<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\MIC_pseudo\\results\\xbd_eval\\mic_full_finetuned_updated\\xbd_mic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD mic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.919244    0.443886        0.472069     0.863670   \n",
      "std    142.971455    0.129163    0.331154        0.319027     0.196842   \n",
      "min      0.000000    0.152405    0.038101        0.233430     0.152405   \n",
      "25%    124.500000    0.887024    0.225168        0.250000     0.804151   \n",
      "50%    246.000000    0.978958    0.248318        0.261962     0.972977   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.453883  \n",
      "std      0.324334  \n",
      "min      0.066125  \n",
      "25%      0.239453  \n",
      "50%      0.249578  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'mic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "--- Starting HIC Evaluation ---\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_baseline_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_baseline_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_baseline_updated: 100%|██████████| 63/63 [09:16<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_baseline_updated\\xbd_hic_baseline_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_baseline_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.994949    0.915736        0.985393     0.923734   \n",
      "std    142.971455    0.020293    0.192093        0.069192     0.181397   \n",
      "min      0.000000    0.785706    0.284500        0.454786     0.333333   \n",
      "25%    124.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "50%    246.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "75%    370.500000    1.000000    1.000000        1.000000     1.000000   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.918539  \n",
      "std      0.188181  \n",
      "min      0.306987  \n",
      "25%      1.000000  \n",
      "50%      1.000000  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'hic_baseline_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_half_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_half_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_half_finetuned_updated: 100%|██████████| 63/63 [09:14<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_half_finetuned_updated\\xbd_hic_half_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_half_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.899208    0.370721        0.410756     0.851788   \n",
      "std    142.971455    0.127586    0.131368        0.114799     0.185653   \n",
      "min      0.000000    0.256775    0.064194        0.241307     0.244811   \n",
      "25%    124.500000    0.857826    0.284344        0.333333     0.784988   \n",
      "50%    246.000000    0.955307    0.327199        0.360690     0.945328   \n",
      "75%    370.500000    0.986343    0.492542        0.500000     0.985924   \n",
      "max    495.000000    1.000000    1.000000        1.000000     1.000000   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.387590  \n",
      "std      0.122742  \n",
      "min      0.102156  \n",
      "25%      0.309790  \n",
      "50%      0.331304  \n",
      "75%      0.496365  \n",
      "max      1.000000  \n",
      "[END EVAL] xBD Model: 'hic_half_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "====================\n",
      "[START EVAL] Model: 'hic_full_finetuned_updated' on xBD tiles.\n",
      "====================\n",
      "Saving xBD metrics to: C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_full_finetuned_updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on xBD: hic_full_finetuned_updated: 100%|██████████| 63/63 [09:13<00:00,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBD metrics saved to C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\\HIC_pseudo\\results\\xbd_eval\\hic_full_finetuned_updated\\xbd_hic_full_finetuned_updated_metrics.csv\n",
      "\n",
      "[SUMMARY: xBD hic_full_finetuned_updated]\n",
      "       tile_index    accuracy    mean_iou  mean_precision  mean_recall  \\\n",
      "count  499.000000  499.000000  499.000000      499.000000   499.000000   \n",
      "mean   247.136273    0.688780    0.194814        0.281192     0.664373   \n",
      "std    142.971455    0.185514    0.065283        0.045014     0.204337   \n",
      "min      0.000000    0.007675    0.001919        0.230965     0.007675   \n",
      "25%    124.500000    0.569176    0.151577        0.250000     0.523003   \n",
      "50%    246.000000    0.705078    0.189602        0.250000     0.684250   \n",
      "75%    370.500000    0.840645    0.236661        0.333333     0.837547   \n",
      "max    495.000000    0.982803    0.336471        0.499666     0.982803   \n",
      "\n",
      "        mean_dice  \n",
      "count  499.000000  \n",
      "mean     0.226622  \n",
      "std      0.059072  \n",
      "min      0.003808  \n",
      "25%      0.190966  \n",
      "50%      0.222011  \n",
      "75%      0.269383  \n",
      "max      0.394849  \n",
      "[END EVAL] xBD Model: 'hic_full_finetuned_updated'.\n",
      "====================\n",
      "\n",
      "\n",
      "All evaluations complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate xBD only\n",
    "#gem ver\n",
    "# ============================\n",
    "# Imports\n",
    "# ============================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "# ============================\n",
    "# Model Definition (U-Net)\n",
    "# ============================\n",
    "class UNetOriginal(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.final = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "# ============================\n",
    "# Helper & Metric Functions\n",
    "# ============================\n",
    "def tile_tensor_and_mask(input_tensor, mask_tensor, tile_size=(256, 256)):\n",
    "    C, H, W = input_tensor.shape\n",
    "    th, tw = tile_size\n",
    "    tiles_input, tiles_mask = [], []\n",
    "    for i in range(0, H, th):\n",
    "        for j in range(0, W, tw):\n",
    "            tiles_input.append(input_tensor[:, i:i+th, j:j+tw])\n",
    "            tiles_mask.append(mask_tensor[i:i+th, j:j+tw])\n",
    "    return tiles_input, tiles_mask\n",
    "\n",
    "def compute_accuracy(pred, target):\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "def compute_all_metrics(pred, target, num_classes=4):\n",
    "    pred_flat = pred.flatten().cpu().numpy()\n",
    "    target_flat = target.flatten().cpu().numpy()\n",
    "    metrics = {'iou': [], 'precision': [], 'recall': [], 'dice': []}\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred_flat == cls\n",
    "        target_inds = target_flat == cls\n",
    "        \n",
    "        intersection = np.logical_and(pred_inds, target_inds).sum()\n",
    "        union = np.logical_or(pred_inds, target_inds).sum()\n",
    "        tp = intersection\n",
    "        fp = np.logical_and(pred_inds, ~target_inds).sum()\n",
    "        fn = np.logical_and(~pred_inds, target_inds).sum()\n",
    "\n",
    "        iou = tp / union if union > 0 else np.nan\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        dice = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else np.nan\n",
    "\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['dice'].append(dice)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# ============================\n",
    "# Dataset Classes\n",
    "# ============================\n",
    "class XBDMulticlassDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_pre=None, transform_post=None,\n",
    "                 image_size=(1024, 1024), tile_size=(256, 256), max_images=None):\n",
    "        self.image_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.files = sorted([f for f in os.listdir(self.image_dir) if '_pre_disaster.png' in f])\n",
    "        if max_images:\n",
    "            self.files = self.files[:max_images]\n",
    "        # DEBUG: Confirm number of images found\n",
    "        print(f\"[XBDDataset] Found {len(self.files)} pre-disaster images in {self.image_dir}\")\n",
    "        self.transform_pre = transform_pre\n",
    "        self.transform_post = transform_post\n",
    "        self.image_size = image_size\n",
    "        self.tile_size = tile_size\n",
    "        self.tiles_per_image = (image_size[0] // tile_size[0]) * (image_size[1] // tile_size[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pre_file = self.files[idx]\n",
    "        post_file = pre_file.replace('pre', 'post')\n",
    "        mask_file = post_file.replace('.png', '_rgb.png')\n",
    "        pre_path = os.path.join(self.image_dir, pre_file)\n",
    "        post_path = os.path.join(self.image_dir, post_file)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "\n",
    "        pre_img_raw = Image.open(pre_path).convert(\"RGB\").resize(self.image_size)\n",
    "        post_img_raw = Image.open(post_path).convert(\"RGB\").resize(self.image_size)\n",
    "        mask_img_raw = Image.open(mask_path).convert(\"RGB\").resize(self.image_size, Image.NEAREST)\n",
    "\n",
    "        def optical_to_sar_like(img):\n",
    "            img = img.convert('L')\n",
    "            img = ImageOps.autocontrast(img, cutoff=2)\n",
    "            return img\n",
    "\n",
    "        def update_mask_multiclass(mask_rgb_img):\n",
    "            mask_np = np.array(mask_rgb_img)\n",
    "            label_mask = np.zeros(mask_np.shape[:2], dtype=np.uint8)\n",
    "            color_to_label = {\n",
    "                (0, 0, 0): 0, (0, 255, 255): 0, (0, 0, 255): 1,\n",
    "                (255, 255, 0): 2, (255, 0, 0): 3, (211, 211, 211): 0\n",
    "            }\n",
    "            for rgb, label in color_to_label.items():\n",
    "                mask = np.all(mask_np == rgb, axis=-1)\n",
    "                label_mask[mask] = label\n",
    "            return label_mask\n",
    "\n",
    "        post_img_sar_raw = optical_to_sar_like(post_img_raw)\n",
    "        pre_img = self.transform_pre(pre_img_raw) if self.transform_pre else pre_img_raw\n",
    "        post_img_sar = self.transform_post(post_img_sar_raw) if self.transform_post else post_img_sar_raw\n",
    "\n",
    "        input_tensor = torch.cat([pre_img, post_img_sar], dim=0)\n",
    "        mask_tensor = torch.tensor(update_mask_multiclass(mask_img_raw), dtype=torch.long)\n",
    "\n",
    "        return tile_tensor_and_mask(input_tensor, mask_tensor, self.tile_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def get_tile_dataset(self):\n",
    "        return TiledXBDDataset(self)\n",
    "\n",
    "class TiledXBDDataset(Dataset):\n",
    "    def __init__(self, parent_dataset):\n",
    "        self.parent_dataset = parent_dataset\n",
    "        self.tiles_per_image = parent_dataset.tiles_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.tiles_per_image\n",
    "        tile_idx = idx % self.tiles_per_image\n",
    "        tiles_input, tiles_mask = self.parent_dataset[image_idx]\n",
    "        return tiles_input[tile_idx], tiles_mask[tile_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parent_dataset) * self.tiles_per_image\n",
    "\n",
    "# Note: The three test datasets are identical. They could be refactored into one class.\n",
    "class BaseTestDataset(Dataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir, dataset_name=\"Test\"):\n",
    "        self.pre_dir = pre_dir\n",
    "        self.post_dir = post_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = [f for f in os.listdir(pre_dir) if f.endswith(('.png', '.jpg'))]\n",
    "        # DEBUG: Confirm number of images found\n",
    "        print(f\"[{dataset_name} Dataset] Found {len(self.files)} images in {pre_dir}\")\n",
    "        self.transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        pre_path = os.path.join(self.pre_dir, fname)\n",
    "        post_path = os.path.join(self.post_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        pre_img = self.transform(Image.open(pre_path).convert(\"RGB\"))\n",
    "        post_img = self.transform(Image.open(post_path).convert(\"L\"))\n",
    "        mask = torch.from_numpy(np.array(Image.open(mask_path))).long()\n",
    "\n",
    "        if post_img.dim() == 2:\n",
    "            post_img = post_img.unsqueeze(0)\n",
    "        \n",
    "        x = torch.cat([pre_img, post_img], dim=0)\n",
    "        \n",
    "        # DEBUG: Check tensor shapes for the first item\n",
    "        if idx == 0:\n",
    "            print(f\"[{self.__class__.__name__}] Sample 0 shapes - Input: {x.shape}, Mask: {mask.shape}\")\n",
    "            \n",
    "        return x, mask, fname\n",
    "\n",
    "class LICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"LIC Test\")\n",
    "\n",
    "class MICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"MIC Test\")\n",
    "\n",
    "class HICTestDataset(BaseTestDataset):\n",
    "    def __init__(self, pre_dir, post_dir, mask_dir):\n",
    "        super().__init__(pre_dir, post_dir, mask_dir, dataset_name=\"HIC Test\")\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_majority_class_per_tile(tile_mask):\n",
    "    # Ignore padding or empty masks\n",
    "    return torch.bincount(tile_mask.flatten()).argmax().item()\n",
    "\n",
    "def stratified_tile_sample(dataset, total_samples=500):\n",
    "    class_indices = {0: [], 1: [], 2: [], 3: []}\n",
    "\n",
    "    print(\"Scanning tile classes for stratified sampling...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, mask = dataset[idx]\n",
    "        majority_class = get_majority_class_per_tile(mask)\n",
    "        if majority_class in class_indices:\n",
    "            class_indices[majority_class].append(idx)\n",
    "\n",
    "    # Count all\n",
    "    total_tiles = sum(len(v) for v in class_indices.values())\n",
    "    proportions = {cls: len(v)/total_tiles for cls, v in class_indices.items()}\n",
    "    print(\"Class proportions:\", proportions)\n",
    "\n",
    "    # Sample proportional counts\n",
    "    sampled_indices = []\n",
    "    for cls in range(4):\n",
    "        n_samples = int(proportions[cls] * total_samples)\n",
    "        n_available = len(class_indices[cls])\n",
    "        chosen = random.sample(class_indices[cls], min(n_samples, n_available))\n",
    "        sampled_indices.extend(chosen)\n",
    "\n",
    "    return Subset(dataset, sampled_indices)\n",
    "\n",
    "# ============================\n",
    "# Evaluation & Visualization\n",
    "# ============================\n",
    "def save_visualization(pre_img, post_img, pred_mask, true_mask, fname, save_dir):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    cmap = mcolors.ListedColormap(['black', 'blue', 'yellow', 'red'])\n",
    "    norm = mcolors.BoundaryNorm(np.arange(-0.5, 4, 1), cmap.N)\n",
    "\n",
    "    axs[0].imshow(pre_img.permute(1, 2, 0).cpu().numpy())\n",
    "    axs[0].set_title(\"Pre-disaster\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(post_img.squeeze(0).cpu().numpy(), cmap='gray')\n",
    "    axs[1].set_title(\"Post SAR\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(true_mask.cpu().numpy(), cmap=cmap, norm=norm)\n",
    "    axs[2].set_title(\"Ground Truth\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    axs[3].imshow(pred_mask.cpu().numpy(), cmap=cmap, norm=norm)\n",
    "    axs[3].set_title(\"Prediction\")\n",
    "    axs[3].axis(\"off\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, fname.replace('.png', '_viz.png')))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_visualize(model, dataloader, device, label, save_root, num_classes=4):\n",
    "    print(f\"\\n{'='*20}\\n[START EVAL] Model: '{label}' on custom test set.\\n{'='*20}\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    per_class_metrics = {cls: {m: [] for m in ['iou', 'precision', 'recall', 'dice']} for cls in range(num_classes)}\n",
    "    vis_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    # DEBUG: Show where results will be saved\n",
    "    print(f\"Saving visualizations and metrics to: {vis_dir}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, fname) in enumerate(tqdm(dataloader, desc=f\"Evaluating {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "            \n",
    "            # DEBUG: Check tensor shapes inside the loop (only for the first batch)\n",
    "            if i == 0:\n",
    "                print(f\"  Batch 0 shapes -> x: {x.shape}, y: {y.shape}, y_pred: {y_pred.shape}\")\n",
    "\n",
    "            for j in range(len(y)):\n",
    "                fname_i = fname[j]\n",
    "                pred_j, y_j = y_pred[j], y[j]\n",
    "                \n",
    "                acc = compute_accuracy(pred_j, y_j)\n",
    "                metrics = compute_all_metrics(pred_j, y_j, num_classes=num_classes)\n",
    "\n",
    "                for cls in range(num_classes):\n",
    "                    for metric_name in per_class_metrics[cls]:\n",
    "                        per_class_metrics[cls][metric_name].append(metrics[metric_name][cls])\n",
    "\n",
    "                results.append({\n",
    "                    \"filename\": fname_i, \"accuracy\": acc,\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "                })\n",
    "                save_visualization(x[j, :3], x[j, 3:], pred_j, y_j, fname_i, vis_dir)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(vis_dir, f\"{label}_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    # DEBUG: Confirm CSV saved\n",
    "    print(f\"Metrics saved to {csv_path}\")\n",
    "\n",
    "    print(f\"\\n[SUMMARY: {label}]\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n[Per-Class Metrics]\")\n",
    "    for cls in range(num_classes):\n",
    "        print(f\"Class {cls}:\")\n",
    "        for metric in ['iou', 'precision', 'recall', 'dice']:\n",
    "            values = np.array(per_class_metrics[cls][metric])\n",
    "            print(f\"  {metric:>9}: mean={np.nanmean(values):.4f}, std={np.nanstd(values):.4f}\")\n",
    "    print(f\"[END EVAL] Model: '{label}'.\\n{'='*20}\")\n",
    "\n",
    "def evaluate_model_on_xbd(model, dataloader, device, label, save_root):\n",
    "    print(f\"\\n{'='*20}\\n[START EVAL] Model: '{label}' on xBD tiles.\\n{'='*20}\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    results = []\n",
    "    save_dir = os.path.join(save_root, label)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # DEBUG: Show where results will be saved\n",
    "    print(f\"Saving xBD metrics to: {save_dir}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(dataloader, desc=f\"Evaluating on xBD: {label}\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(x), dim=1)\n",
    "\n",
    "            for j in range(x.size(0)):\n",
    "                pred_j, y_j = y_pred[j], y[j]\n",
    "                metrics = compute_all_metrics(pred_j, y_j, num_classes=4)\n",
    "                results.append({\n",
    "                    \"tile_index\": i * x.size(0) + j,\n",
    "                    \"accuracy\": compute_accuracy(pred_j, y_j),\n",
    "                    \"mean_iou\": np.nanmean(metrics['iou']),\n",
    "                    \"mean_precision\": np.nanmean(metrics['precision']),\n",
    "                    \"mean_recall\": np.nanmean(metrics['recall']),\n",
    "                    \"mean_dice\": np.nanmean(metrics['dice']),\n",
    "})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(save_dir, f\"xbd_{label}_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    # DEBUG: Confirm CSV saved\n",
    "    print(f\"xBD metrics saved to {csv_path}\")\n",
    "\n",
    "    print(f\"\\n[SUMMARY: xBD {label}]\")\n",
    "    print(df.describe())\n",
    "    print(f\"[END EVAL] xBD Model: '{label}'.\\n{'='*20}\")\n",
    "\n",
    "# ============================\n",
    "# Main Execution\n",
    "# ============================\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Step 1: Define Paths ---\n",
    "    base_dir = r\"C:\\Users\\sweta\\anaconda_projects\\non-trivial\\performance_bias\"\n",
    "    \n",
    "    # Model paths\n",
    "    iteration_models_dir = os.path.join(base_dir, \"iteration_models\")\n",
    "    baseline_path = os.path.join(iteration_models_dir, \"model_epoch8.pt\")\n",
    "    \n",
    "    lic_half_path = os.path.join(iteration_models_dir, \"lic_half_finetunedmodel.pt\")\n",
    "    lic_full_path = os.path.join(iteration_models_dir, \"lic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    mic_half_path = os.path.join(iteration_models_dir, \"mic_half_finetunedmodel.pt\")\n",
    "    mic_full_path = os.path.join(iteration_models_dir, \"mic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    hic_half_path = os.path.join(iteration_models_dir, \"hic_half_finetunedmodel.pt\")\n",
    "    hic_full_path = os.path.join(iteration_models_dir, \"hic_full_finetunedmodel.pt\")\n",
    "    \n",
    "    # Data paths\n",
    "    xbd_root = r\"C:\\Users\\sweta\\.cache\\kagglehub\\datasets\\qianlanzz\\xbd-dataset\\versions\\1\\xbd\\tier1\"\n",
    "    lic_test_dir = os.path.join(base_dir, \"LIC_pseudo\", \"test\")\n",
    "    mic_test_dir = os.path.join(base_dir, \"MIC_pseudo\", \"test\")\n",
    "    hic_test_dir = os.path.join(base_dir, \"HIC_pseudo\", \"test\")\n",
    "\n",
    "    # Results paths\n",
    "    lic_results_root = os.path.join(base_dir, \"LIC_pseudo\", \"results\")\n",
    "    mic_results_root = os.path.join(base_dir, \"MIC_pseudo\", \"results\")\n",
    "    hic_results_root = os.path.join(base_dir, \"HIC_pseudo\", \"results\")\n",
    "\n",
    "    # --- Step 2: Load Models ---\n",
    "    def load_model_from_path(path):\n",
    "        print(f\"Loading model from: {path}\")\n",
    "        model = UNetOriginal(in_channels=4, out_classes=4)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"  [ERROR] Model path does not exist: {path}\")\n",
    "            return None\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        # Handle both dict and checkpoint objects\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            # DEBUG\n",
    "            print(\"  Loading from a checkpoint object...\")\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            # DEBUG\n",
    "            print(\"  Loading from a raw state dictionary...\")\n",
    "            model.load_state_dict(checkpoint)\n",
    "        return model\n",
    "\n",
    "    models_to_test = {\n",
    "        \"LIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(lic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(lic_full_path),\n",
    "        },\n",
    "        \"MIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(mic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(mic_full_path),\n",
    "        },\n",
    "        \"HIC\": {\n",
    "            \"baseline\": load_model_from_path(baseline_path),\n",
    "            \"half_finetuned\": load_model_from_path(hic_half_path),\n",
    "            \"full_finetuned\": load_model_from_path(hic_full_path),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # --- Step 3: Prepare DataLoaders ---\n",
    "    # Create xBD loader (used for all evaluations)\n",
    "    transform_pre = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    transform_post = T.Compose([T.ToTensor(), T.Normalize(mean=[0.5], std=[0.5])])\n",
    "    from torch.utils.data import Subset\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    xbd_tile_dataset = xbd_dataset.get_tile_dataset()\n",
    "    stratified_subset = stratified_tile_sample(xbd_tile_dataset, total_samples=500)\n",
    "    xbd_loader = DataLoader(stratified_subset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "    # Create LIC, MIC, HIC loaders\n",
    "    lic_test_dataset = LICTestDataset(os.path.join(lic_test_dir, \"pre\"), os.path.join(lic_test_dir, \"post\"), os.path.join(lic_test_dir, \"mask\"))\n",
    "    lic_loader = DataLoader(lic_test_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    mic_test_dataset = MICTestDataset(os.path.join(mic_test_dir, \"pre\"), os.path.join(mic_test_dir, \"post\"), os.path.join(mic_test_dir, \"mask\"))\n",
    "    mic_loader = DataLoader(mic_test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    hic_test_dataset = HICTestDataset(os.path.join(hic_test_dir, \"pre\"), os.path.join(hic_test_dir, \"post\"), os.path.join(hic_test_dir, \"mask\"))\n",
    "    hic_loader = DataLoader(hic_test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "    # --- Step 4: Run Evaluations ---\n",
    "    \n",
    "    # Evaluate on LIC\n",
    "    print(\"\\n\\n--- Starting LIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"LIC\"].items():\n",
    "        if model:\n",
    "          #  evaluate_and_visualize(model, lic_loader, device, label=f\"lic_{name}_updated\", save_root=lic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"lic_{name}_updated\", save_root=os.path.join(lic_results_root, \"xbd_eval\"))\n",
    "\n",
    "    # Evaluate on MIC\n",
    "    print(\"\\n\\n--- Starting MIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"MIC\"].items():\n",
    "        if model:\n",
    "           # evaluate_and_visualize(model, mic_loader, device, label=f\"mic_{name}_updated\", save_root=mic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"mic_{name}_updated\", save_root=os.path.join(mic_results_root, \"xbd_eval\"))\n",
    "            \n",
    "    # Evaluate on HIC\n",
    "    print(\"\\n\\n--- Starting HIC Evaluation ---\")\n",
    "    for name, model in models_to_test[\"HIC\"].items():\n",
    "        if model:\n",
    "           # evaluate_and_visualize(model, hic_loader, device, label=f\"hic_{name}_updated\", save_root=hic_results_root)\n",
    "            evaluate_model_on_xbd(model, xbd_loader, device, label=f\"hic_{name}_updated\", save_root=os.path.join(hic_results_root, \"xbd_eval\"))\n",
    "\n",
    "    print(\"\\n\\nAll evaluations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c53e2c5-4204-4f99-b9bc-427b37d49b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sweta\\\\anaconda_projects\\\\non-trivial\\\\performance_bias'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdb616-230d-4e10-a28f-eecede665acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
